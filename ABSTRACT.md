# VISIOPE_project

## ABSTRACT

**URBE Perception: real-time vehicle detection for self-driving cars in Rome**

Edge computing, in particular Edge AI, is the deployment of AI applications on embedded devices. It's called "edge" because all the computations are done near the user at the edge of the network, close to where the data are located, rather than centrally in a cloud computing facility or data center. Due to the maturation of neural networks and to the advances reached in compute infrastructure, it's now possible to deploy AI applications "on the edge" to solve real-world problems, since AI algorithms are capable of understanding language, sights, faces, objects and other analog forms of unstructured information. 

Organizations from every industry are looking nowadays to increase automation and intelligence in their systems to improve efficiency and safety. To help them, computer programs need to execute tasks repeatedly and in a secure manner. Such jobs would be impractical to deploy on the cloud computing due to issues related to latency, bandwidth and privacy. That's why today the interest in edge computing is rising more and over. For machines to replicate human intelligence, it's required a deep neural network to be trained. After training, the network becomes an “inference engine” that can answer real-world questions, and it's exactly the inference time (that is the time of the network to make a prediction) that can play a fundamental role in an AI application. So, in all those real-time applications where the inference time is crucial, making the computation on the edge is needed. An example can be find in the field of Autonomous Driving.

As it can be imagined, the computational system behind a self-driving car is huge and extremely complex; it integrates many technologies, including sensing (lidars, cameras, radars), localization, decision making and perception (objects recognition and tracking). This is very challenging, since the design goal of autonomous driving edge computing systems is to guarantee the safety of the vehicles themselves and they need to process an enormous amount of data in real time with extremely tight latency constraints. 

For instance, if an autonomous vehicle travels at 50 km/h, and thus about 30 m of braking distance, this requires the autonomous driving system to predict potential dangers up to a few seconds before they occur.
Imagine a situation in which the vehicle has few instants to recognize if is about to clash with another car; the right recognition at the right time could prevent the passengers from doing a car accident or not. Therefore, the faster the autonomous driving edge computing system performs these complex computations (inference time), the safer the autonomous vehicle is.

This project will then focus on the perception capability of an autonomous car, in particular on an object detection and recognition submodule that can be found in a real "autonomous driving algorithms system". The objects that are going to be detected and classified are **vehicles** (cars, trucks and buses), **people** and **motorbikes**. Even though it may seem simple, there can be some factors that make it a task that is not at all obvious: the environment may change because of different weather conditions and the objects may be influenced by illumination variations, perspectives or occlusions.

Fortunately in recent years, as it's known, we have seen the rapid development of deep learning technology, which achieves significant results in object detection and recognition. Especially thanks to convolutional neural networks. These networks can be huge in terms of learnt parameters, that can easily reach the millions of them. Both this aspect and the level of operations complexity involved in them, leads to a very high inference time, that is the most important aspect when dealing with object recognition in real-time (that is, while the car is driving autonomously). 

So, the main two goals of this project will be to: first, reach a good performance on the detection and recognition task and second, decrease the inference time by manipulating and simplify somehow the  neural network under consideration. To achieve this I'll first create a custom dataset using three already existing datasets (**Waymo**, **BDD100K** and **Argoverse-HD**) in order to generalize as musch as possible and then implement a custom YOLO model from scratch (based on the **YOLOv5** version that is the best documented) using the *pytorch-lightning* framework. For improving the inference time and the Frame Per Seconds measurements I will leverage both *pruning* and *quantization* techiniques.