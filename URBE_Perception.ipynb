{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckG8WeM-Dgpa"
      },
      "source": [
        "# **URBE *Perception*** ðŸš˜ - *real-time vehicle detection for self-driving cars in Rome*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNF3FMZaI-95"
      },
      "source": [
        "> *Refer to the notebook* [![ðŸ“”](https://colab.reasearch.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sCqnwYm9Dodk1YodD1asVpRMBBdT-8r1#scrollTo=4teaWmm61Fbl) *on **dataset** creation if you haven't already.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why edge computing and self-driving cars?**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The significance of edge computing in self-driving cars lies in its ability to process data and make decisions in real-time at the edge of the network, near the source of the data. This is imperative for self-driving cars, as they require prompt and precise decision-making based on information obtained from sensors such as cameras, radar, and lidar.\n",
        "\n",
        "In a cloud computing software architecture, data is sent to a central server for processing and then the results are sent back to the device. This approach doesn't work for self-driving cars, as the latency, or delay, of transmitting data back and forth between the car and a central server can be dangerous in a real-time driving scenario. With edge computing, the data is processed locally on the car, reducing latency and improving response times. It is indeed a key technology for enabling self-driving cars to operate safely and effectively.\n",
        "\n",
        "The popularity of this approach to constructing intelligent systems is growing, especially in light of a potentially challenging future in terms of the scarcity of materials for performing extensive computations. For this reason many researchers in the AI field are pursuing this direction (https://news.mit.edu/2023/autonomous-vehicles-carbon-emissions-0113)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The idea**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it can be imagined, the computational system behind a self-driving car is huge and extremely complex; it integrates many technologies, including sensing (lidars, cameras, radars), localization, decision making and **perception** on which my work is focused. <br> **Urbe** stands for \"*city*\" and it is used to be referred to the city of Rome. In fact my final goal is to build a real-time system which runs on embedded devices (as *Nvidia Jetson Nano* or *Google Coral* ) and which detects *vehicles*, *pedestrians* and *motorbikes* on the streets of Rome. In effect, a real submodule for autonomous cars. But for this project I only built an object detection system with an eye toward  the  inference time and most importantly towards the application scenario, Rome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHj5qBhNpaJ"
      },
      "source": [
        "## Imports & Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install the requirements\n",
        "%pip install -r requirements.txt > /dev/null\n",
        "# set to false if you already have the dataset\n",
        "download_dataset = False \n",
        "if download_dataset:\n",
        "    %cd dataset\n",
        "    !bash download_dataset.sh\n",
        "    %cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.hyperparameters import Hparams\n",
        "from src.data_module import URBE_DataModule\n",
        "from src.model import URBE_Perception\n",
        "from src.loss import YOLO_Loss\n",
        "from src.train import train_model\n",
        "\n",
        "from dataclasses import asdict\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import json\n",
        "import torchvision.transforms as T\n",
        "import pytorch_lightning as pl\n",
        "import gc\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# reproducibility stuff\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "_ = pl.seed_everything(0)\n",
        "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
        "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#%env WANDB_NOTEBOOK_NAME = ./notebook.ipynb\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# login wandb to have the online logger. It is really useful since it stores all the plots and evolution of the model\n",
        "# check also https://docs.wandb.ai/guides/integrations/lightning\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McjXDHbZNROw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ciUHB0NV9a",
        "outputId": "e1a4baf9-8050-4cbe-e455-dbf05ea3c23e"
      },
      "outputs": [],
      "source": [
        "# fast check to see if all the data were correctly imported\n",
        "print(\"We should get the same number in both three cases...\")\n",
        "l1 = os.listdir(\"dataset/URBE_dataset/images/train\") + os.listdir(\"dataset/URBE_dataset/images/test\") + os.listdir(\"dataset/URBE_dataset/images/val\")\n",
        "print(len(l1))\n",
        "\n",
        "d=json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "l2 = list(set([ann[\"image_id\"] for ann in d[\"annotations\"]]))\n",
        "print(len(l2))\n",
        "\n",
        "l3 = d[\"images\"]\n",
        "print(len(l3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "URBE_Data = URBE_DataModule(hparams)\n",
        "# to setup it takes ~6 minutes\n",
        "URBE_Data.setup()\n",
        "print(len(URBE_Data.data_train)) # --> 3500 images\n",
        "print(len(URBE_Data.data_val)) # -->  438 images\n",
        "print(len(URBE_Data.data_test)) # -->  438 images\n",
        "print(\"TOTAL: \"+str(len(URBE_Data.data_train)+len(URBE_Data.data_val)+len(URBE_Data.data_test))+\" images\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FurxaRlU86tq"
      },
      "source": [
        "### Bounding Boxes Visualization\n",
        "\n",
        "It's needed of course for showing the results at the end of the project and during  training of the validation set, but it was essential in the *data processing* phase for understanding the qualities of the datasets' bounding boxes annotations and in general to recognize each different characteristic of the data. <br> *(I tried **Scalabel**, **FiftyOne**, but **WandB** is the best choice)* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Let's test the *dataloaders* and see some samples from a training batch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bbox(label):\n",
        "  ris = { \"predictions\" : {\"box_data\" : [] , \"class_labels\" : {0 : \"vehicle\" , 1 : \"person\", 2 : \"motorbike\"}} }\n",
        "  for ann in label: # for each bbox of the particular image\n",
        "    if ann.sum()==0: # we appended this [0,0,0,0,0] type of list for having the same batch size for all the samples!\n",
        "      break\n",
        "    position = {\"minX\": ann[0], \"maxX\": ann[0] + ann[2], \"minY\": ann[1], \"maxY\": ann[1] + ann[3]}\n",
        "    class_id = ann[4]\n",
        "    box_caption = ris[\"predictions\"][\"class_labels\"][class_id]\n",
        "    x = {\"position\" : position, \"domain\" : \"pixel\", \"class_id\" : class_id, \"box_caption\" : box_caption}\n",
        "    ris[\"predictions\"][\"box_data\"].append(x)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we take one batch from the training set\n",
        "batch = next(iter(URBE_Data.train_dataloader()))\n",
        "\n",
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"prova\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "transform = T.ToPILImage()\n",
        "images_list = [transform(img) for img in batch[\"img\"]]\n",
        "\n",
        "my_data = []\n",
        "for i,label in enumerate(batch[\"labels\"]):\n",
        "    bbox_list = draw_bbox(label) # label is a list of lists\n",
        "    my_data.append([batch[\"id\"][i], wandb.Image(images_list[i], boxes=bbox_list)])\n",
        "table = wandb.Table(columns=['ID', 'Image'], data=my_data)\n",
        "print(\"logging the table...\")\n",
        "wandb.log({\"dataloaders testing\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkZIRDeNw_q"
      },
      "source": [
        "### Statistics ðŸ“Š"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SerbGxNhg09P"
      },
      "source": [
        "Before starting with the real development of the detection system, we want to plot the statistics of our data. \n",
        "> Since using the dataloaders  for all our dataset is costly and painful, it will be use the \"*annotations.json*\" file as a source for the dataset statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for plotting data --> three group because train/val/test\n",
        "def three_group_bar(columns, data, title, percentage=True): # both columns and data are lists (data is list of a single list)\n",
        "  labels = columns\n",
        "  \n",
        "  train = data[0]\n",
        "  val = data[1]\n",
        "  test = data[2]\n",
        "  \n",
        "  color_list = []\n",
        "  for _ in range(len(data)):\n",
        "    color = [random.randrange(0, 255)/255, random.randrange(0, 255)/255, random.randrange(0, 255)/255, 1]\n",
        "    color_list.append(color)\n",
        "    \n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.15  # the width of the bars\n",
        "  fig, ax = plt.subplots(figsize=(12, 5), layout='constrained')\n",
        "  rects1 = ax.bar(x - width, train, width, label='Train', color=color_list[0])\n",
        "  rects2 = ax.bar(x, val, width, label='Val', color=color_list[1])\n",
        "  rects3 = ax.bar(x + width, test, width, label='Test', color=color_list[2])\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_title(title)\n",
        "  ax.set_xticks(x, labels)\n",
        "  ax.legend()\n",
        "  if percentage:\n",
        "    rects1_labels = [('%.2f' % i) + \"%\" for i in train]\n",
        "    rects2_labels = [('%.2f' % i) + \"%\" for i in val]\n",
        "    rects3_labels = [('%.2f' % i) + \"%\" for i in test]\n",
        "  else:\n",
        "    rects1_labels = train\n",
        "    rects2_labels = val\n",
        "    rects3_labels = test\n",
        "  \n",
        "  ax.bar_label(rects1, rects1_labels, padding=3)\n",
        "  ax.bar_label(rects2, rects2_labels, padding=3)\n",
        "  ax.bar_label(rects3, rects3_labels, padding=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup\n",
        "d = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "annotations = d[\"annotations\"]\n",
        "images = d[\"images\"]\n",
        "\n",
        "train_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/train/\")]\n",
        "val_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/val/\")]\n",
        "test_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/test/\")]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in train_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# VAL\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in val_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# TEST\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in test_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[82.46814899865309, 17.16686171034909, 0.364989290997814], [82.85404948638728, 16.814104764671193, 0.33184574894152646], [82.80439305749428, 16.843565364727365, 0.35204157777836304]]\n",
        "columns = [\"vehicle\", \"person\", \"motorbike\"]\n",
        "three_group_bar(columns, data, \"train/val/test Classes Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2khwW5WH0dqM"
      },
      "source": [
        "**Time of the day**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in train_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# VAL\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in val_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# TEST\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in test_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[57.89522657485811, 35.27379733879222, 6.830976086349678], [58.056361763879785, 34.95927347626627, 6.984364759853946], [58.73741141365162, 34.61395001864976, 6.64863856769862]]\n",
        "columns = [\"day\", \"night\", \"dawn/dusk\"]\n",
        "three_group_bar(columns, data, \"train/val/test TimeOfDay Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhhybij_bds"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6oUMXIyQ5L"
      },
      "source": [
        "We organized the dataset in order to be compatible with the COCO dataset. We did it initially because all the *YOLO* architectures were trained/tested on it.\n",
        "\n",
        "We'll now focus more on the **YOLOv5** model, considered one of the best ones at the moment in terms of the  *accuracy*/*time inference* trade-off and with a very *pytorch* detailed documentation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to achieve the best performances on our custom \"*URBE_dataset*\". In order to realize this we need to perform the following steps:\n",
        "- build a custom YOLOv5 architecture (based on the official repo), to be able to use the *pretrained weights* on the COCO dataset for the **backbone** and the **neck** part. \n",
        "- thanks to the **autoanchor** algorithm implemented by Glenn Jocher (one of the authors of YOLOv5), we compute the best new anchors that fit our dataset. This contributes significantly to enhancing the overall model.\n",
        "- adding only basic augmentations on the images and also on the bounding boxes thanks to Albumentation library. We decided to not apply **Mosaic Augmenation** (that is one of main suggested augmentation techniques for YOLOv5) because the custom dataset already conveys to the model a big enough generalization capability.\n",
        "- trying to attach the **Decoupled Head** at the end of model (as it was added in YOLOv6 and subsequent architectures) and see if there's an improvement.\n",
        "- playing around with different versions of the **IoU loss** (GIoU, DIoU or CIoU)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The *git repos* from which I took some informations about building the model architecture are:\n",
        "- https://github.com/ultralytics/yolov5\n",
        "- https://github.com/AlessandroMondin/YOLOV5m\n",
        "- https://github.com/Iywie/pl_YOLO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoanchor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors in YOLO models are predefined bounding boxes used to represent the shape and size of the objects in an image. These anchors are used as a reference to compare the predicted bounding boxes from the model with the actual bounding boxes around the objects. \n",
        "\n",
        "Glenn Jocher introduced the idea of learning anchor boxes based on the distribution of bounding boxes in the custom dataset with *K-means* and *genetic* learning algorithms. This is very important for custom tasks, because the distribution of bounding box sizes and locations may be dramatically different than the preset bounding box anchors in the COCO dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> *The autoanchor algorithm is automatically computed before training (train code made publicly available by the YOLOv5 authors) starts.* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are indeed going to make the annotations (that are in COCO format) compatible with the \"*YOLOv5 text format*\". Then we are going to \"train\" a YOLOv5 architecture on our dataset (but actually we'll only leverage the functionality of autoanchor method). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "annotations = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\", \"r\"))[\"annotations\"]\n",
        "\n",
        "# save image_id for each images in dataset/YOLOv5_format/train and also the .txt name where the labels will be written!\n",
        "image_id_list = []\n",
        "txt_labels_list = []\n",
        "for f in os.listdir(\"dataset/YOLOv5_format/images/1\")+os.listdir(\"dataset/YOLOv5_format/images/2\")+os.listdir(\"dataset/YOLOv5_format/images/3\")+os.listdir(\"dataset/YOLOv5_format/images/4\"):\n",
        "    image_id = f.split(\"_\")[-1][:-4]\n",
        "    txt_labels_name = f[:-4]+\".txt\"\n",
        "    image_id_list.append(image_id)\n",
        "    txt_labels_list.append(txt_labels_name)\n",
        "\n",
        "# filtering of only the annotations of the images in dataset/YOLOv5_format/train\n",
        "filter_annotations = list(filter(lambda x: x[\"image_id\"] in image_id_list, annotations))\n",
        "\n",
        "# for each images in dataset/YOLOv5_format/train\n",
        "for image_id, txt_labels_name in list(zip(image_id_list, txt_labels_list)):\n",
        "    image_labels = list( map(lambda x: [x[\"category_id\"], x[\"bbox\"][0], x[\"bbox\"][1], x[\"bbox\"][2], x[\"bbox\"][3]], list(filter(lambda x: x[\"image_id\"] == image_id, filter_annotations)) ) )\n",
        "    \n",
        "    txt_file_name = \"dataset/YOLOv5_format/labels/\" + txt_labels_name\n",
        "\n",
        "    line_to_write = []\n",
        "    for line in image_labels:\n",
        "        x1 = float(line[1])\n",
        "        y1 = float(line[2])\n",
        "        w = float(line[3])\n",
        "        h = float(line[4])\n",
        "        c1 = round(((x1 + w/2) / 1280), 2)\n",
        "        c2 = round(((y1 + h/2) / 720), 2)\n",
        "        w = round(w/1280, 2)\n",
        "        h = round(h/720, 2)\n",
        "        line_to_write.append(\" \".join([str(line[0]), str(c1), str(c2), str(w), str(h)]))\n",
        "    with open(txt_file_name, 'w') as f:\n",
        "        f.write(\"\\n\".join(line_to_write))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> <a href=\"https://imgur.com/Iebkt2Y\"><img src=\"https://i.imgur.com/Iebkt2Y.png\" width=65 height=25 title=\"source: imgur.com\" /></a> After downloading the YOLOv5 format dataset on *Roboflow*, we \"train\" it using the YOLOv5 official code."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We followed the [colab Roboflow tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb) about training YOLOv5 with a dataset already uploaded in Roboflow in order to perform the autoanchor algorithm and see if the default given anchors fit the dataset. The answer was positive and therefore we didn't have to change them. \n",
        "\n",
        "<a href=\"https://imgur.com/zy6z9o9\"><img src=\"https://i.imgur.com/zy6z9o9.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretrained weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aim is to load the *pretrained* weights of YOLOv5 architecture in our models. We create four \"*.pt*\" files because we have to deal with two versions of the YOLOv5 model (**medium** and **small**) and with two different types of HEADs (**Simple** and **Decoupled**). For this reason, we're going to load only the pretrained weights of the BACKBONE and the NECK. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../yolov5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5m**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_decoupled.pt\")\n",
        "model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_decoupled.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#user_name = \"lavallone\"\n",
        "#project_name = \"VISIOPE_project\"\n",
        "#version_name = \"prova\"\n",
        "#run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "data = URBE_DataModule(hparams)\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "if hparams[\"load_pretrained\"]:\n",
        "    if hparams[\"first_out\"] == 48:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))\n",
        "    else:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))\n",
        "            \n",
        "# RESUME logic is embedded within the trainer\n",
        "# trainer = train_model(data, model, experiment_name = version_name, \\\n",
        "#    patience=5, metric_to_monitor=\"mAP_50\", mode=\"max\", epochs = 10)\n",
        "\n",
        "\n",
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.setup()\n",
        "model.to(\"cuda\")\n",
        "#loss_fn = YOLO_Loss(hparams)\n",
        "for batch in iter(data.train_dataloader()):\n",
        "    with torch.no_grad():\n",
        "        images = batch[\"img\"].to(\"cuda\")\n",
        "        out = model(images)\n",
        "        labels = batch[\"labels\"]\n",
        "        ris = model.predict(out, labels)\n",
        "        print(ris[\"mAP\"][0][\"boxes\"].shape)\n",
        "        print(ris[\"mAP\"][1][\"boxes\"].shape)\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_performance(model, data):\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    dataset = data.test_dataloader() # TEST SET\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mAP_list = []\n",
        "        for i, batch in enumerate(tqdm(iter(dataset))): # tqdm ci permette di visualizzare il progresso della lettura del dataset\n",
        "            imgs = batch['img']\n",
        "            out = model(imgs)\n",
        "            \n",
        "            targets = [YOLO_Loss.transform_targets(out, bboxes, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.hparams.ignore_iou_thresh) for bboxes in targets]\n",
        "            # I want targets to be the same shape as predictions --> (bs, 3 , 80/40/20, 80/40/20, 6)\n",
        "            t1 = torch.stack([target[0] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t2 = torch.stack([target[1] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t3 = torch.stack([target[2] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            targets = [t1, t2, t3]\n",
        "            \n",
        "            pred_boxes = model.cells_to_bboxes(out, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.device,  is_pred=True, to_list=False)\n",
        "            true_boxes = model.cells_to_bboxes(targets, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.device, is_pred=False, to_list=False)\n",
        "            pred_boxes = model.non_max_suppression(pred_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, tolist=False, max_detections=300)\n",
        "            true_boxes = model.non_max_suppression(true_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, tolist=False, max_detections=300)\n",
        "            \n",
        "            pred_dict = dict(boxes=pred_boxes[..., 2:], scores=pred_boxes[..., 1], labels=pred_boxes[..., 0],)\n",
        "            true_dict = dict(boxes=true_boxes[..., 2:], labels=true_boxes[..., 0],)\n",
        "            \n",
        "            mAP = MeanAveragePrecision(iou_thresholds=None) # in this way the IoU thresholds are taken from the stepped range [0.5,...,0.95] with step 0.05\n",
        "            mAP.update(pred_dict, true_dict)\n",
        "            \n",
        "            mAP_50_95 = mAP.compute()[\"map\"]\n",
        "            mAP_50 = mAP.compute()[\"map_50\"]\n",
        "            \n",
        "            print(f\"Batch: {i}\")\n",
        "            print(f\"mAP_50_95: {mAP_50_95:.3f}\")\n",
        "            print(f\"mAP_50: {mAP_50:.3f}\")\n",
        "            mAP_list.append((mAP_50_95, mAP_50))\n",
        "        \n",
        "        print()\n",
        "        all_map_50_95 = [e[0] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@0.5: {np.array(all_map_50_95).mean()}\")\n",
        "        print()\n",
        "        all_map_50 = [e[1] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@[0.5:0.95]: {np.array(all_map_50).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_ckpt = True\n",
        "if load_ckpt:\n",
        "    best_ckpt = \"models/prova-epoch=00-val_ROUGE=0.6224.ckpt\"\n",
        "    model = URBE_Perception.load_from_checkpoint(best_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# if we want to test without training we need to setup the data\n",
        "trained = False \n",
        "if not trained:\n",
        "    hparams = asdict(Hparams())\n",
        "    data = URBE_DataModule(hparams)\n",
        "    data.setup()\n",
        "\n",
        "evaluate_performance(model, data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ultimate objective is to create a *real-time detection model* that can be integrated into the intricate self-driving system software architecture. In addition to performance metrics, the inference time of the model is also crucial, perhaps even more so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how to correctly compute inference time (therefore fps) for a model\n",
        "# https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "dummy_input = torch.randn(1, 3, 640, 640, dtype=torch.float).to(device)\n",
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "repetitions = 300\n",
        "timings = np.zeros((repetitions,1))\n",
        "\n",
        "# GPU-WARM-UP\n",
        "for _ in range(10):\n",
        "   _ = model(dummy_input)\n",
        "   \n",
        "# MEASURE PERFORMANCE\n",
        "with torch.no_grad():\n",
        "  for rep in range(repetitions):\n",
        "     starter.record()\n",
        "     _ = model(dummy_input)\n",
        "     ender.record()\n",
        "     # WAIT FOR GPU SYNC\n",
        "     torch.cuda.synchronize()\n",
        "     curr_time = starter.elapsed_time(ender)\n",
        "     timings[rep] = curr_time\n",
        "\n",
        "mean_syn = np.sum(timings) / repetitions\n",
        "inference_time = mean_syn/1000\n",
        "fps = 1/inference_time\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Parameters: {num_param}\")\n",
        "print(f\"Inference time: {inference_time:.3f} seconds\")\n",
        "print(f\"Frame Per Second: {fps:.3f}\")\n",
        "print(\"---------------------------------------\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategies for decrease Inference Time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mainly leverage three methods:\n",
        "\n",
        "- **Mixed precision (16 bit) Training**. Lower precision, such as the 16-bit floating-point, enables the training and deployment of large neural networks since they require less memory and they run faster ( achieving upto +3X speedups on modern GPUs).\n",
        "- **Pruning**. We'll use the *l1_unstructured* pruning method which acts by zeroing out the units with the lowest L1-norm. In addition to that, there is in pytorch lightning the possibility to easily use the *lottery ticket hypothesis* (https://arxiv.org/abs/1803.03635). \n",
        "- **Quantization**. Model quantization is another performance optimization technique that allows speeding up inference and decreasing memory requirements by performing computations and storing tensors at lower bitwidths than floating-point precision. This is particularly beneficial during model deployment. We use *Quantization Aware Training (QAT)*, which mimics the effects of quantization during training: the computations are carried-out in floating-point precision but the subsequent quantization effect is taken into account. The weights and activations are quantized into lower precision only for inference, when training is completed.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> One thing we need to try is the *fusion* between batch normalization layer and the activation function layer. Apparently it conveys faster inference!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorRT <a href=\"https://imgur.com/Unhn3jE\"><img width=55 height= 25 src=\"https://i.imgur.com/Unhn3jE.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorRT is a deep learning inference optimizer and runtime library developed by NVIDIA. It is designed to optimize and accelerate the inference of deep learning models on NVIDIA GPUs, making it suitable for deployment in various embedded environments. Considering our application for self-driving cars and the need for real-time inference, this is a step we cannot do without.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We try to convert our trained models to TensorRT format and run inference on them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "!python setup.py install\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "import torch\n",
        "from torch2trt import torch2trt\n",
        "\n",
        "model.to(\"cuda\")\n",
        "# create example data\n",
        "x = torch.zeros((1, 3, 640, 640)).cuda()\n",
        "\n",
        "# convert to TensorRT feeding sample data as input\n",
        "model_trt = torch2trt(model, [x]) # we can now execute the returned TRTModule just like the original PyTorch model :)\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE \n",
        "torch.save(model_trt.state_dict(), 'model_trt.pth')\n",
        "\n",
        "# and LOAD\n",
        "%cd ../torch2trt\n",
        "from torch2trt import TRTModule\n",
        "model_trt = TRTModule()\n",
        "model_trt.load_state_dict(torch.load('model.pth'))\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th><center> Model </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50-95</t> </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b1</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b32</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> params <br><t style=\"font-size:12px;\">(M)</t> </center></th>\n",
        "  <tr>\n",
        "    <td><center><i>URBE YOLOv5m</center></td>\n",
        "    <td><center>57.30</center></td>\n",
        "    <td><center>82.86</center></td>\n",
        "    <td><center>57.71</center></td>\n",
        "    <td><center>20 <i>~ 10 FPS</i></center></td>\n",
        "    <td><center>55.33</center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i>URBE YOLOv5m</center></td>\n",
        "    <td><center>80.83</center></td>\n",
        "    <td><center>94.35</center></td>\n",
        "    <td><center>80.54</center></td>\n",
        "    <th><center>95 <i>~ 30 FPS</i></center></th>\n",
        "    <th><center>79.33</center></th>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORS = np.array([\n",
        "                    [173, 255, 47],\n",
        "                    [186, 85, 211],\n",
        "                    [255, 215, 0]\n",
        "                  ])\n",
        "\n",
        "def vis(img, boxes, scores, cls_ids, conf=0.5, class_names={0 : \"vehicle\", 1 : \"person\", 2 : \"motorbike\"}):\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        cls_id = int(cls_ids[i])\n",
        "        score = scores[i]\n",
        "        if score < conf:\n",
        "            continue\n",
        "        x0 = int(box[0])\n",
        "        y0 = int(box[1])\n",
        "        x1 = int(box[2])\n",
        "        y1 = int(box[3])\n",
        "\n",
        "        color = (COLORS[cls_id]).astype(np.uint8).tolist()\n",
        "        text = '{} : {:.1f}'.format(class_names[cls_id], score * 100)\n",
        "        txt_color = (0, 0, 0)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        \n",
        "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
        "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
        "\n",
        "        txt_bk_color = (COLORS[cls_id] * 0.7).astype(np.uint8).tolist()\n",
        "        cv2.rectangle(\n",
        "                        img,\n",
        "                        (x0, y0 + 1),\n",
        "                        (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
        "                        txt_bk_color,\n",
        "                        -1\n",
        "                     )\n",
        "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_frame(output, img_info, cls_conf=0.35): # tool che ci fa visualizzare le bboxes!\n",
        "    ratio = img_info[\"ratio\"]\n",
        "    img = img_info[\"raw_img\"]\n",
        "    if output is None: # devo capire come ci si comporta se non ci sono prediction per la singola immagine!!!\n",
        "        return img # questo Ã¨ quando non ci sono predizioni\n",
        "    output = output.cpu()\n",
        "    \n",
        "    bboxes = output[:, 0:4]\n",
        "\n",
        "    # sta roba del artio Ã¨ giÃ  inclusa nel mio postprocessing\n",
        "    #bboxes /= ratio\n",
        "\n",
        "    cls = output[:, 5]\n",
        "    scores = output[:, 4]\n",
        "\n",
        "    vis_res = vis(img, bboxes, scores, cls, cls_conf)\n",
        "    return vis_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_inference(model, img):\n",
        "    img_info = {\"id\": 0}\n",
        "    if isinstance(img, str):\n",
        "        img_info[\"file_name\"] = os.path.basename(img)\n",
        "        img = cv2.imread(img)\n",
        "    else:\n",
        "        img_info[\"file_name\"] = None\n",
        "    \n",
        "    height, width = img.size\n",
        "    img_info[\"height\"] = height\n",
        "    img_info[\"width\"] = width\n",
        "    img_info[\"raw_img\"] = img\n",
        "\n",
        "    ratio = min(model.hparams.img_size / img.size[0], model.hparams.img_size / img.size[1])\n",
        "    img_info[\"ratio\"] = ratio\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "                                    transforms.Resize((model.hparams.img_size, model.hparams.img_size)),\n",
        "                                    transforms.ToTensor()\n",
        "                                   ])\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    img = img.float()\n",
        "    img = img.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        # outputs = postprocess( # Ã¨ il processo per fare le predictions\n",
        "        #     outputs, self.num_classes, self.confthre,\n",
        "        #     self.nmsthre, class_agnostic=True\n",
        "        # )\n",
        "        outputs = torch.tensor([ [[50, 70, 400, 500, 0.99, 0], [60, 70, 300, 200, 0.99, 1], [60, 40, 200, 100, 0.80, 2]] ])\n",
        "    return outputs, img_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_results = True\n",
        "\n",
        "# tutta roba che serve se vogliamo salvare un video\n",
        "cap = cv2.VideoCapture(\"video/Streets_of_Rome.mp4\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "if save_results:\n",
        "    video_writer = cv2.VideoWriter(\"video/results/ris.mp4\", cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\"), 30, (1280, 720)) # fps and dimension of the output video is set\n",
        "    \n",
        "# per ogni frame nel video    \n",
        "while True:\n",
        "    ret_val, frame = cap.read()\n",
        "    if ret_val:\n",
        "        # convert from np.array to PIL Image for inference\n",
        "        img = Image.fromarray(frame)\n",
        "        outputs, img_info = make_inference(model, img)\n",
        "        # but for visualization I don't need PIL Image\n",
        "        img_info[\"raw_img\"] = frame\n",
        "        result_frame = visualize_frame(outputs[0], img_info, model.hparams.conf_threshold)\n",
        "        if save_results:\n",
        "            video_writer.write(frame)\n",
        "        else:\n",
        "            cv2.namedWindow(\"Urbe Perception\", cv2.WINDOW_NORMAL)\n",
        "            cv2.imshow(\"Urbe Perception\", result_frame)\n",
        "            ch = cv2.waitKey(30) # 50 milliseconds per frame --> FPS~20\n",
        "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "                break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "McjXDHbZNROw",
        "FurxaRlU86tq",
        "KgUD3Gw764ei",
        "hXkZIRDeNw_q",
        "eJhhybij_bds"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sappia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "5f06e66338bb5301debc8a4cff3b178f3ee2a0c1aca00670585ce1ed8b6c95da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
