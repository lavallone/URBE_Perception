{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckG8WeM-Dgpa"
      },
      "source": [
        "# **URBE *Perception*** ðŸš˜ - *real-time vehicle detection for self-driving cars in Rome*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNF3FMZaI-95"
      },
      "source": [
        "> *Refer to the notebook* [![ðŸ“”](https://colab.reasearch.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sCqnwYm9Dodk1YodD1asVpRMBBdT-8r1#scrollTo=4teaWmm61Fbl) *on **dataset** creation if you haven't already.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why edge computing and self-driving cars?**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The significance of edge computing in self-driving cars lies in its ability to process data and make decisions in real-time at the edge of the network, near the source of the data. This is imperative for self-driving cars, as they require prompt and precise decision-making based on information obtained from sensors such as cameras, radar, and lidar.\n",
        "\n",
        "In a cloud computing software architecture, data is sent to a central server for processing and then the results are sent back to the device. This approach doesn't work for self-driving cars, as the latency, or delay, of transmitting data back and forth between the car and a central server can be dangerous in a real-time driving scenario. With edge computing, the data is processed locally on the car, reducing latency and improving response times. It is indeed a key technology for enabling self-driving cars to operate safely and effectively.\n",
        "\n",
        "The popularity of this approach to constructing intelligent systems is growing, especially in light of a potentially challenging future in terms of the scarcity of materials for performing extensive computations. For this reason many researchers in the AI field are pursuing this direction (https://news.mit.edu/2023/autonomous-vehicles-carbon-emissions-0113)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The idea**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it can be imagined, the computational system behind a self-driving car is huge and extremely complex; it integrates many technologies, including sensing (lidars, cameras, radars), localization, decision making and **perception** on which my work is focused. <br> **Urbe** stands for \"*city*\" and it is used to be referred to the city of Rome. In fact my final goal is to build a real-time system which runs on embedded devices (as *Nvidia Jetson Nano* or *Google Coral* ) and which detects *vehicles*, *pedestrians* and *motorbikes* on the streets of Rome. In effect, a real submodule for autonomous cars. But for this project I only built an object detection system with an eye toward  the  inference time and most importantly towards the application scenario, Rome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHj5qBhNpaJ"
      },
      "source": [
        "## Imports & Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install the requirements\n",
        "%pip install -r requirements.txt > /dev/null\n",
        "# set to false if you already have the dataset\n",
        "download_dataset = False \n",
        "if download_dataset:\n",
        "    %cd dataset\n",
        "    !bash download_dataset.sh\n",
        "    %cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from src.hyperparameters import Hparams\n",
        "from src.data_module import URBE_DataModule\n",
        "from src.model import URBE_Perception\n",
        "from src.loss import YOLO_Loss\n",
        "from src.train import train_model\n",
        "\n",
        "from dataclasses import asdict\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import json\n",
        "import torchvision.transforms as T\n",
        "import pytorch_lightning as pl\n",
        "import gc\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# reproducibility stuff\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "_ = pl.seed_everything(0)\n",
        "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
        "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#%env WANDB_NOTEBOOK_NAME = ./notebook.ipynb\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# login wandb to have the online logger. It is really useful since it stores all the plots and evolution of the model\n",
        "# check also https://docs.wandb.ai/guides/integrations/lightning\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McjXDHbZNROw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ciUHB0NV9a",
        "outputId": "e1a4baf9-8050-4cbe-e455-dbf05ea3c23e"
      },
      "outputs": [],
      "source": [
        "# fast check to see if all the data were correctly imported\n",
        "print(\"We should get the same number in both three cases...\")\n",
        "l1 = os.listdir(\"dataset/URBE_dataset/images/train\") + os.listdir(\"dataset/URBE_dataset/images/test\") + os.listdir(\"dataset/URBE_dataset/images/val\")\n",
        "print(len(l1))\n",
        "\n",
        "d=json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "l2 = list(set([ann[\"image_id\"] for ann in d[\"annotations\"]]))\n",
        "print(len(l2))\n",
        "\n",
        "l3 = d[\"images\"]\n",
        "print(len(l3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "URBE_Data = URBE_DataModule(hparams)\n",
        "# to setup it takes ~6 minutes\n",
        "URBE_Data.setup()\n",
        "print(len(URBE_Data.data_train)) # --> 3500 images\n",
        "print(len(URBE_Data.data_val)) # -->  438 images\n",
        "print(len(URBE_Data.data_test)) # -->  438 images\n",
        "print(\"TOTAL: \"+str(len(URBE_Data.data_train)+len(URBE_Data.data_val)+len(URBE_Data.data_test))+\" images\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FurxaRlU86tq"
      },
      "source": [
        "### Bounding Boxes Visualization\n",
        "\n",
        "It's needed of course for showing the results at the end of the project and during  training of the validation set, but it was essential in the *data processing* phase for understanding the qualities of the datasets' bounding boxes annotations and in general to recognize each different characteristic of the data. <br> *(I tried **Scalabel**, **FiftyOne**, but **WandB** is the best choice)* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Let's test the *dataloaders* and see some samples from a training batch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bbox(label):\n",
        "  ris = { \"predictions\" : {\"box_data\" : [] , \"class_labels\" : {0 : \"vehicle\" , 1 : \"person\", 2 : \"motorbike\"}} }\n",
        "  for ann in label: # for each bbox of the particular image\n",
        "    if ann.sum()==0: # we appended this [0,0,0,0,0] type of list for having the same batch size for all the samples!\n",
        "      break\n",
        "    position = {\"minX\": (ann[1]*1280).item(), \"maxX\": ((ann[1] + ann[3])*1280).item(), \"minY\": (ann[2]*720).item(), \"maxY\": ((ann[2] + ann[4])*720).item()}\n",
        "    class_id = int(ann[0])\n",
        "    box_caption = ris[\"predictions\"][\"class_labels\"][class_id]\n",
        "    x = {\"position\" : position, \"domain\" : \"pixel\", \"class_id\" : class_id, \"box_caption\" : box_caption}\n",
        "    ris[\"predictions\"][\"box_data\"].append(x)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we take one batch from the training set\n",
        "batch = next(iter(URBE_Data.train_dataloader()))\n",
        "\n",
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"prova\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "transform = T.ToPILImage()\n",
        "images_list = [transform(img) for img in batch[\"img\"]]\n",
        "images_list = [img.resize((1280, 720)) for img in images_list]\n",
        "\n",
        "my_data = []\n",
        "for i,label in enumerate(batch[\"labels\"]):\n",
        "    bbox_list = draw_bbox(label) # label is a list of lists\n",
        "    my_data.append([batch[\"id\"][i], wandb.Image(images_list[i], boxes=bbox_list)])\n",
        "table = wandb.Table(columns=['ID', 'Image'], data=my_data)\n",
        "print(\"logging the table...\")\n",
        "wandb.log({\"dataloaders testing\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkZIRDeNw_q"
      },
      "source": [
        "### Statistics ðŸ“Š"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SerbGxNhg09P"
      },
      "source": [
        "Before starting with the real development of the detection system, we want to plot the statistics of our data. \n",
        "> Since using the dataloaders  for all our dataset is costly and painful, it will be use the \"*annotations.json*\" file as a source for the dataset statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for plotting data --> three group because train/val/test\n",
        "def three_group_bar(columns, data, title, percentage=True): # both columns and data are lists (data is list of a single list)\n",
        "  labels = columns\n",
        "  \n",
        "  train = data[0]\n",
        "  val = data[1]\n",
        "  test = data[2]\n",
        "  \n",
        "  color_list = []\n",
        "  for _ in range(len(data)):\n",
        "    color = [random.randrange(0, 255)/255, random.randrange(0, 255)/255, random.randrange(0, 255)/255, 1]\n",
        "    color_list.append(color)\n",
        "    \n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.15  # the width of the bars\n",
        "  fig, ax = plt.subplots(figsize=(12, 5), layout='constrained')\n",
        "  rects1 = ax.bar(x - width, train, width, label='Train', color=color_list[0])\n",
        "  rects2 = ax.bar(x, val, width, label='Val', color=color_list[1])\n",
        "  rects3 = ax.bar(x + width, test, width, label='Test', color=color_list[2])\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_title(title)\n",
        "  ax.set_xticks(x, labels)\n",
        "  ax.legend()\n",
        "  if percentage:\n",
        "    rects1_labels = [('%.2f' % i) + \"%\" for i in train]\n",
        "    rects2_labels = [('%.2f' % i) + \"%\" for i in val]\n",
        "    rects3_labels = [('%.2f' % i) + \"%\" for i in test]\n",
        "  else:\n",
        "    rects1_labels = train\n",
        "    rects2_labels = val\n",
        "    rects3_labels = test\n",
        "  \n",
        "  ax.bar_label(rects1, rects1_labels, padding=3)\n",
        "  ax.bar_label(rects2, rects2_labels, padding=3)\n",
        "  ax.bar_label(rects3, rects3_labels, padding=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup\n",
        "d = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "annotations = d[\"annotations\"]\n",
        "images = d[\"images\"]\n",
        "\n",
        "train_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/train/\")]\n",
        "val_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/val/\")]\n",
        "test_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/test/\")]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in train_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# VAL\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in val_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# TEST\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in test_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[82.46814899865309, 17.16686171034909, 0.364989290997814], [82.85404948638728, 16.814104764671193, 0.33184574894152646], [82.80439305749428, 16.843565364727365, 0.35204157777836304]]\n",
        "columns = [\"vehicle\", \"person\", \"motorbike\"]\n",
        "three_group_bar(columns, data, \"train/val/test Classes Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2khwW5WH0dqM"
      },
      "source": [
        "**Time of the day**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in train_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# VAL\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in val_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# TEST\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in test_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[57.89522657485811, 35.27379733879222, 6.830976086349678], [58.056361763879785, 34.95927347626627, 6.984364759853946], [58.73741141365162, 34.61395001864976, 6.64863856769862]]\n",
        "columns = [\"day\", \"night\", \"dawn/dusk\"]\n",
        "three_group_bar(columns, data, \"train/val/test TimeOfDay Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhhybij_bds"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6oUMXIyQ5L"
      },
      "source": [
        "We organized the dataset in order to be compatible with the COCO dataset. We did it initially because all the *YOLO* architectures were trained/tested on it.\n",
        "\n",
        "We'll now focus more on the **YOLOv5** model, considered one of the best ones at the moment in terms of the  *accuracy*/*time inference* trade-off and with a very *pytorch* detailed documentation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to achieve the best performances on our custom \"*URBE_dataset*\". In order to realize this we need to perform the following steps:\n",
        "- build a custom YOLOv5 architecture (based on the official repo), to be able to use the *pretrained weights* on the COCO dataset for the **backbone** and the **neck** part. \n",
        "- thanks to the **autoanchor** algorithm implemented by Glenn Jocher (one of the authors of YOLOv5), we compute the best new anchors that fit our dataset. This contributes significantly to enhancing the overall model.\n",
        "- adding only basic augmentations on the images and also on the bounding boxes thanks to Albumentation library. We decided to not apply **Mosaic Augmenation** (that is one of main suggested augmentation techniques for YOLOv5) because the custom dataset already conveys to the model a big enough generalization capability.\n",
        "- trying to attach the **Decoupled Head** at the end of model (as it was added in YOLOv6 and subsequent architectures) and see if there's an improvement.\n",
        "- playing around with different versions of the **IoU loss** (GIoU, DIoU or CIoU)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The *git repos* from which I took some informations about building the model architecture are:\n",
        "- https://github.com/ultralytics/yolov5\n",
        "- https://github.com/AlessandroMondin/YOLOV5m\n",
        "- https://github.com/Iywie/pl_YOLO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoanchor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors in YOLO models are predefined bounding boxes used to represent the shape and size of the objects in an image. These anchors are used as a reference to compare the predicted bounding boxes from the model with the actual bounding boxes around the objects. \n",
        "\n",
        "Glenn Jocher introduced the idea of learning anchor boxes based on the distribution of bounding boxes in the custom dataset with *K-means* and *genetic* learning algorithms. This is very important for custom tasks, because the distribution of bounding box sizes and locations may be dramatically different than the preset bounding box anchors in the COCO dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> *The autoanchor algorithm is automatically computed before training (train code made publicly available by the YOLOv5 authors) starts.* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are indeed going to make the annotations (that are in COCO format) compatible with the \"*YOLOv5 text format*\". Then we are going to \"train\" a YOLOv5 architecture on our dataset (but actually we'll only leverage the functionality of autoanchor method). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "annotations = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\", \"r\"))[\"annotations\"]\n",
        "\n",
        "# save image_id for each images in dataset/YOLOv5_format/train and also the .txt name where the labels will be written!\n",
        "image_id_list = []\n",
        "txt_labels_list = []\n",
        "for f in os.listdir(\"dataset/YOLOv5_format/images/1\")+os.listdir(\"dataset/YOLOv5_format/images/2\")+os.listdir(\"dataset/YOLOv5_format/images/3\")+os.listdir(\"dataset/YOLOv5_format/images/4\"):\n",
        "    image_id = f.split(\"_\")[-1][:-4]\n",
        "    txt_labels_name = f[:-4]+\".txt\"\n",
        "    image_id_list.append(image_id)\n",
        "    txt_labels_list.append(txt_labels_name)\n",
        "\n",
        "# filtering of only the annotations of the images in dataset/YOLOv5_format/train\n",
        "filter_annotations = list(filter(lambda x: x[\"image_id\"] in image_id_list, annotations))\n",
        "\n",
        "# for each images in dataset/YOLOv5_format/train\n",
        "for image_id, txt_labels_name in list(zip(image_id_list, txt_labels_list)):\n",
        "    image_labels = list( map(lambda x: [x[\"category_id\"], x[\"bbox\"][0], x[\"bbox\"][1], x[\"bbox\"][2], x[\"bbox\"][3]], list(filter(lambda x: x[\"image_id\"] == image_id, filter_annotations)) ) )\n",
        "    \n",
        "    txt_file_name = \"dataset/YOLOv5_format/labels/\" + txt_labels_name\n",
        "\n",
        "    line_to_write = []\n",
        "    for line in image_labels:\n",
        "        x1 = float(line[1])\n",
        "        y1 = float(line[2])\n",
        "        w = float(line[3])\n",
        "        h = float(line[4])\n",
        "        c1 = round(((x1 + w/2) / 1280), 2)\n",
        "        c2 = round(((y1 + h/2) / 720), 2)\n",
        "        w = round(w/1280, 2)\n",
        "        h = round(h/720, 2)\n",
        "        line_to_write.append(\" \".join([str(line[0]), str(c1), str(c2), str(w), str(h)]))\n",
        "    with open(txt_file_name, 'w') as f:\n",
        "        f.write(\"\\n\".join(line_to_write))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> <a href=\"https://imgur.com/Iebkt2Y\"><img src=\"https://i.imgur.com/Iebkt2Y.png\" width=65 height=25 title=\"source: imgur.com\" /></a> After downloading the YOLOv5 format dataset on *Roboflow*, we \"train\" it using the YOLOv5 official code."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We followed the [colab Roboflow tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb) about training YOLOv5 with a dataset already uploaded in Roboflow in order to perform the autoanchor algorithm and see if the default given anchors fit the dataset. The answer was positive and therefore we didn't have to change them. \n",
        "\n",
        "<a href=\"https://imgur.com/zy6z9o9\"><img src=\"https://i.imgur.com/zy6z9o9.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretrained weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aim is to load the *pretrained* weights of YOLOv5 architecture in our models. We create four \"*.pt*\" files because we have to deal with two versions of the YOLOv5 model (**medium** and **small**) and with two different types of HEADs (**Simple** and **Decoupled**). For this reason, we're going to load only the pretrained weights of the BACKBONE and the NECK. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../yolov5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5m**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_decoupled.pt\")\n",
        "model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_decoupled.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavallone\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.10 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/lavallone/Desktop/VISIOPE/VISIOPE_project/wandb/run-20230224_174922-cm8i7jjc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lavallone/VISIOPE_project/runs/cm8i7jjc\" target=\"_blank\">yolov5_m</a></strong> to <a href=\"https://wandb.ai/lavallone/VISIOPE_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lavallone/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "  rank_zero_warn(\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:41<00:00, 12.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading val dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:05<00:00, 11.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:05<00:00, 12.26it/s]\n",
            "/home/lavallone/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/lavallone/Desktop/VISIOPE/VISIOPE_project/models exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type                 | Params\n",
            "--------------------------------------------------\n",
            "0 | backbone | Backbone             | 12.2 M\n",
            "1 | neck     | Neck                 | 8.7 M \n",
            "2 | head     | SimpleHead           | 32.3 K\n",
            "3 | mAP      | MeanAveragePrecision | 0     \n",
            "--------------------------------------------------\n",
            "8.7 M     Trainable params\n",
            "12.2 M    Non-trainable params\n",
            "20.9 M    Total params\n",
            "83.518    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "755fe66f1d114f06aef2fef396ae1ea9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706b13074e524777bb423532aeffbd15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n",
            "no predictions!!!!\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m             model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mpretrained/yolov5n_nh_decoupled.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[39m# RESUME logic is embedded within the trainer\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m trainer \u001b[39m=\u001b[39m train_model(data, model, experiment_name \u001b[39m=\u001b[39m version_name, \\\n\u001b[1;32m     24\u001b[0m    patience\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, metric_to_monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmAP_50\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m     26\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
            "File \u001b[0;32m~/Desktop/VISIOPE/VISIOPE_project/src/train.py:39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data, model, experiment_name, patience, metric_to_monitor, mode, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     35\u001b[0m         logger\u001b[39m=\u001b[39mlogger, max_epochs\u001b[39m=\u001b[39mepochs, log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, gpus\u001b[39m=\u001b[39mn_gpus,\n\u001b[1;32m     36\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, precision \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mprecision,\n\u001b[1;32m     37\u001b[0m         num_sanity_val_steps\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     38\u001b[0m         )\n\u001b[0;32m---> 39\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 768\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    770\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    720\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    722\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    805\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    806\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    807\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m )\n\u001b[0;32m--> 809\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    811\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    812\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1234\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1236\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1350\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:268\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    265\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:255\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:309\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    153\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 154\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    156\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    126\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    128\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1763\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1765\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Desktop/VISIOPE/VISIOPE_project/src/model.py:546\u001b[0m, in \u001b[0;36mURBE_Perception.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    543\u001b[0m    \u001b[39m#self.log(\"val_accuracy_class\", (pred[\"accuracy\"][1] / (pred[\"accuracy\"][0] + 1e-16)), on_step=True, on_epoch=True, prog_bar=True, batch_size=self.hparams.batch_size)\u001b[39;00m\n\u001b[1;32m    544\u001b[0m    \u001b[39m#self.log(\"val_accuracy_obj\", (pred[\"accuracy\"][3] / (pred[\"accuracy\"][2] + 1e-16)), on_step=True, on_epoch=True, prog_bar=True, batch_size=self.hparams.batch_size)\u001b[39;00m\n\u001b[1;32m    545\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmAP\u001b[39m.\u001b[39mupdate(pred[\u001b[39m\"\u001b[39m\u001b[39mmAP\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m], pred[\u001b[39m\"\u001b[39m\u001b[39mmAP\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 546\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mmAP_50\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmAP\u001b[39m.\u001b[39;49mcompute()[\u001b[39m\"\u001b[39m\u001b[39mmap_50\u001b[39m\u001b[39m\"\u001b[39m], on_step\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mbatch_size) \u001b[39m# in caso metti self.mAP[\"map_50\"]\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \t\u001b[39m# IMAGES\u001b[39;00m\n\u001b[1;32m    549\u001b[0m    \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mlog_image_each_epoch \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torchmetrics/metric.py:530\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_context(\n\u001b[1;32m    526\u001b[0m     dist_sync_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    527\u001b[0m     should_sync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync,\n\u001b[1;32m    528\u001b[0m     should_unsync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_unsync,\n\u001b[1;32m    529\u001b[0m ):\n\u001b[0;32m--> 530\u001b[0m     value \u001b[39m=\u001b[39m compute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m _squeeze_if_scalar(value)\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torchmetrics/detection/mean_ap.py:905\u001b[0m, in \u001b[0;36mMeanAveragePrecision.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m\"\"\"Compute the `Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)` scores.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \n\u001b[1;32m    880\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m        - mar_100_per_class: ``torch.Tensor`` (-1 if class metrics are disabled)\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_classes()\n\u001b[1;32m    906\u001b[0m     precisions, recalls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate(classes)\n\u001b[1;32m    907\u001b[0m     map_val, mar_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summarize_results(precisions, recalls)\n",
            "File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torchmetrics/detection/mean_ap.py:439\u001b[0m, in \u001b[0;36mMeanAveragePrecision._get_classes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m\"\"\"Returns a list of unique classes found in ground truth and detection data.\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_labels) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroundtruth_labels) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetection_labels \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroundtruth_labels)\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    440\u001b[0m \u001b[39mreturn\u001b[39;00m []\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)"
          ]
        }
      ],
      "source": [
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"yolov5_m\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "data = URBE_DataModule(hparams)\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "if hparams[\"load_pretrained\"]:\n",
        "    if hparams[\"first_out\"] == 48:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))\n",
        "    else:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))\n",
        "            \n",
        "# RESUME logic is embedded within the trainer\n",
        "trainer = train_model(data, model, experiment_name = version_name, \\\n",
        "   patience=20, metric_to_monitor=\"mAP_50\", mode=\"max\", epochs = 1)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[3, 8], []]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l = []\n",
        "l.append([3,8])\n",
        "l.append([])\n",
        "l"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NO TRAINING - testing playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_image(image, labels):\n",
        "    \n",
        "    COLORS = np.array([\n",
        "                    [173, 255, 47],\n",
        "                    [186, 85, 211],\n",
        "                    [255, 215, 0]\n",
        "                  ])\n",
        "    \n",
        "    class_names={0 : \"vehicle\", 1 : \"person\", 2 : \"motorbike\"}\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        box = labels[i]\n",
        "        cls_id = int(box[0])\n",
        "        score = 1\n",
        "        x0 = int(box[1] *1280)\n",
        "        y0 = int(box[2] *720)\n",
        "        x1 = int((box[3]) *1280)\n",
        "        y1 = int((box[4]) *720)\n",
        "\n",
        "        color = (COLORS[cls_id]).astype(np.uint8).tolist()\n",
        "        text = '{} : {:.1f}'.format(class_names[cls_id], score * 100)\n",
        "        txt_color = (0, 0, 0)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        \n",
        "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
        "        cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)\n",
        "\n",
        "        txt_bk_color = (COLORS[cls_id] * 0.7).astype(np.uint8).tolist()\n",
        "        cv2.rectangle(\n",
        "                        image,\n",
        "                        (x0, y0 + 1),\n",
        "                        (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
        "                        txt_bk_color,\n",
        "                        -1\n",
        "                     )\n",
        "        cv2.putText(image, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
        "            \n",
        "\n",
        "    cv2.imshow(\"sto cazzo\", image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "data = URBE_DataModule(hparams)\n",
        "model = URBE_Perception(hparams)\n",
        "data.setup()\n",
        "model.to(\"cuda\")\n",
        "\n",
        "for batch in iter(data.val_dataloader()):\n",
        "    with torch.no_grad():\n",
        "        images = batch[\"img\"].to(\"cuda\")\n",
        "        out = model(images)\n",
        "        labels = batch[\"labels\"]\n",
        "        print(batch[\"file_name\"])\n",
        "        image = cv2.imread(batch[\"file_name\"][0], cv2.IMREAD_COLOR)\n",
        "        #show_image(image, labels[0])\n",
        "        print(labels)\n",
        "        print()\n",
        "        t = [YOLO_Loss.transform_targets(out, bboxes, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, 0.5) for bboxes in labels]\n",
        "        \n",
        "        t1 = torch.stack([target[0] for target in t], dim=0).to(\"cuda\",non_blocking=True)\n",
        "        t2 = torch.stack([target[1] for target in t], dim=0).to(\"cuda\",non_blocking=True)\n",
        "        t3 = torch.stack([target[2] for target in t], dim=0).to(\"cuda\",non_blocking=True)\n",
        "        targets = [t1, t2, t3] # ognuno Ã¨ (bs, 3, 80, 80, 6)\n",
        "        \n",
        "        t1 = torch.reshape(t1, (1, 3*80*80, 6))\n",
        "        for b in range(t1.shape[0]):\n",
        "            for i in range(t1.shape[1]):\n",
        "                if t1[b, i, :].sum()==0:\n",
        "                    continue\n",
        "                print(t1[b, i, :])\n",
        "        print()      \n",
        "        t2 = torch.reshape(t2, (1, 3*40*40, 6))\n",
        "        for b in range(t2.shape[0]):\n",
        "            for i in range(t2.shape[1]):\n",
        "                if t2[b, i, :].sum()==0:\n",
        "                    continue\n",
        "                print(t2[b, i, :])\n",
        "        print()\n",
        "        t3 = torch.reshape(t3, (1, 3*20*20, 6))\n",
        "        for b in range(t3.shape[0]):\n",
        "            for i in range(t3.shape[1]):\n",
        "                if t3[b, i, :].sum()==0:\n",
        "                    continue\n",
        "                print(t3[b, i, :])\n",
        "        print()\n",
        "        \n",
        "        # INIZIA IL TESTING su cell_to_boxes\n",
        "        for i in range(3):\n",
        "            grid = [torch.empty(0) for _ in range(3)]  # initialize\n",
        "            anchor_grid = [torch.empty(0) for _ in range(3)]  # initialize\n",
        "            strides = [8,16,32]\n",
        "            bs, naxs, ny, nx, _ = targets[i].shape # (bs, 3, 80/40/20, 80/40/20, _)\n",
        "            stride = strides[i] # 8/16/32\n",
        "            grid[i], anchor_grid[i] = model.make_grids(torch.tensor(URBE_Perception.ANCHORS), naxs, ny=ny, nx=nx, i=i)\n",
        "            \n",
        "            torch.count_nonzero(targets[i][..., 4:5]) # 3 sono le bounding boxes al primo livello di scala\n",
        "            obj = targets[i][..., 4:5] == 1\n",
        "            obj = torch.cat([obj,obj], -1)\n",
        "            best_class = targets[i][..., 5:6]\n",
        "            xy = ((targets[i][..., 0:2] + grid[i]) * stride) [obj]\n",
        "            xy = torch.reshape(xy, (3,2))\n",
        "            wh = (targets[i][..., 2:4] * stride) [obj]\n",
        "            wh = torch.reshape(wh, (3,2))\n",
        "            labels = torch.cat([torch.zeros(3,1).to(\"cuda\"),xy, wh],-1)/640\n",
        "            #show_image(image, labels)\n",
        "            \n",
        "        x, pred = model.predict(out, batch[\"labels\"])\n",
        "        classes = x[0][:, 0:1]\n",
        "        x = x[0][:, 2:] / 640\n",
        "        labels = torch.cat([classes, x], -1)\n",
        "        show_image(image, labels)\n",
        "        \n",
        "        mAP = MeanAveragePrecision()\n",
        "        mAP.update(pred[\"mAP\"][0], pred[\"mAP\"][1])\n",
        "        ris = mAP.compute()[\"map_50\"]\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_performance(model, data):\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    dataset = data.test_dataloader() # TEST SET\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mAP_list = []\n",
        "        for i, batch in enumerate(tqdm(iter(dataset))): # tqdm ci permette di visualizzare il progresso della lettura del dataset\n",
        "            imgs = batch['img']\n",
        "            out = model(imgs)\n",
        "            \n",
        "            targets = [YOLO_Loss.transform_targets(out, bboxes, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.hparams.ignore_iou_thresh) for bboxes in targets]\n",
        "            # I want targets to be the same shape as predictions --> (bs, 3 , 80/40/20, 80/40/20, 6)\n",
        "            t1 = torch.stack([target[0] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t2 = torch.stack([target[1] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t3 = torch.stack([target[2] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            targets = [t1, t2, t3]\n",
        "            \n",
        "            pred_boxes = model.cells_to_bboxes(out, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.device,  is_pred=True, to_list=False)\n",
        "            true_boxes = model.cells_to_bboxes(targets, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE, model.device, is_pred=False, to_list=False)\n",
        "            pred_boxes = model.non_max_suppression(pred_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, tolist=False, max_detections=300)\n",
        "            true_boxes = model.non_max_suppression(true_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, tolist=False, max_detections=300)\n",
        "            \n",
        "            pred_dict = dict(boxes=pred_boxes[..., 2:], scores=pred_boxes[..., 1], labels=pred_boxes[..., 0],)\n",
        "            true_dict = dict(boxes=true_boxes[..., 2:], labels=true_boxes[..., 0],)\n",
        "            \n",
        "            mAP = MeanAveragePrecision(iou_thresholds=None) # in this way the IoU thresholds are taken from the stepped range [0.5,...,0.95] with step 0.05\n",
        "            mAP.update(pred_dict, true_dict)\n",
        "            \n",
        "            mAP_50_95 = mAP.compute()[\"map\"]\n",
        "            mAP_50 = mAP.compute()[\"map_50\"]\n",
        "            \n",
        "            print(f\"Batch: {i}\")\n",
        "            print(f\"mAP_50_95: {mAP_50_95:.3f}\")\n",
        "            print(f\"mAP_50: {mAP_50:.3f}\")\n",
        "            mAP_list.append((mAP_50_95, mAP_50))\n",
        "        \n",
        "        print()\n",
        "        all_map_50_95 = [e[0] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@0.5: {np.array(all_map_50_95).mean()}\")\n",
        "        print()\n",
        "        all_map_50 = [e[1] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@[0.5:0.95]: {np.array(all_map_50).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_ckpt = True\n",
        "if load_ckpt:\n",
        "    best_ckpt = \"models/prova-epoch=00-val_ROUGE=0.6224.ckpt\"\n",
        "    model = URBE_Perception.load_from_checkpoint(best_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# if we want to test without training we need to setup the data\n",
        "trained = False \n",
        "if not trained:\n",
        "    hparams = asdict(Hparams())\n",
        "    data = URBE_DataModule(hparams)\n",
        "    data.setup()\n",
        "\n",
        "evaluate_performance(model, data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ultimate objective is to create a *real-time detection model* that can be integrated into the intricate self-driving system software architecture. In addition to performance metrics, the inference time of the model is also crucial, perhaps even more so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how to correctly compute inference time (therefore fps) for a model\n",
        "# https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "dummy_input = torch.randn(1, 3, 640, 640, dtype=torch.float).to(device)\n",
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "repetitions = 300\n",
        "timings = np.zeros((repetitions,1))\n",
        "\n",
        "# GPU-WARM-UP\n",
        "for _ in range(10):\n",
        "   _ = model(dummy_input)\n",
        "   \n",
        "# MEASURE PERFORMANCE\n",
        "with torch.no_grad():\n",
        "  for rep in range(repetitions):\n",
        "     starter.record()\n",
        "     _ = model(dummy_input)\n",
        "     ender.record()\n",
        "     # WAIT FOR GPU SYNC\n",
        "     torch.cuda.synchronize()\n",
        "     curr_time = starter.elapsed_time(ender)\n",
        "     timings[rep] = curr_time\n",
        "\n",
        "mean_syn = np.sum(timings) / repetitions\n",
        "inference_time = mean_syn/1000\n",
        "fps = 1/inference_time\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Parameters: {num_param}\")\n",
        "print(f\"Inference time: {inference_time:.3f} seconds\")\n",
        "print(f\"Frame Per Second: {fps:.3f}\")\n",
        "print(\"---------------------------------------\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategies for decrease Inference Time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mainly leverage three methods:\n",
        "\n",
        "- **Mixed precision (16 bit) Training**. Lower precision, such as the 16-bit floating-point, enables the training and deployment of large neural networks since they require less memory and they run faster ( achieving upto +3X speedups on modern GPUs).\n",
        "- **Pruning**. We'll use the *l1_unstructured* pruning method which acts by zeroing out the units with the lowest L1-norm. In addition to that, there is in pytorch lightning the possibility to easily use the *lottery ticket hypothesis* (https://arxiv.org/abs/1803.03635). \n",
        "- **Quantization**. Model quantization is another performance optimization technique that allows speeding up inference and decreasing memory requirements by performing computations and storing tensors at lower bitwidths than floating-point precision. This is particularly beneficial during model deployment. We use *Quantization Aware Training (QAT)*, which mimics the effects of quantization during training: the computations are carried-out in floating-point precision but the subsequent quantization effect is taken into account. The weights and activations are quantized into lower precision only for inference, when training is completed.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorRT <a href=\"https://imgur.com/Unhn3jE\"><img width=55 height= 25 src=\"https://i.imgur.com/Unhn3jE.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorRT is a deep learning inference optimizer and runtime library developed by NVIDIA. It is designed to optimize and accelerate the inference of deep learning models on NVIDIA GPUs, making it suitable for deployment in various embedded environments. Considering our application for self-driving cars and the need for real-time inference, this is a step we cannot do without.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We try to convert our trained models to TensorRT format and run inference on them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "!python setup.py install\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "import torch\n",
        "from torch2trt import torch2trt\n",
        "\n",
        "model.to(\"cuda\")\n",
        "# create example data\n",
        "x = torch.zeros((1, 3, 640, 640)).cuda()\n",
        "\n",
        "# convert to TensorRT feeding sample data as input\n",
        "model_trt = torch2trt(model, [x]) # we can now execute the returned TRTModule just like the original PyTorch model :)\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE \n",
        "torch.save(model_trt.state_dict(), 'model_trt.pth')\n",
        "\n",
        "# and LOAD\n",
        "%cd ../torch2trt\n",
        "from torch2trt import TRTModule\n",
        "model_trt = TRTModule()\n",
        "model_trt.load_state_dict(torch.load('model.pth'))\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th><center> Model </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50-95</t> </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b1</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b32</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> params <br><t style=\"font-size:12px;\">(M)</t> </center></th>\n",
        "  <tr>\n",
        "    <td><center><i>URBE YOLOv5m</center></td>\n",
        "    <td><center>57.30</center></td>\n",
        "    <td><center>82.86</center></td>\n",
        "    <td><center>57.71</center></td>\n",
        "    <td><center>20 <i>~ 10 FPS</i></center></td>\n",
        "    <td><center>55.33</center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i>URBE YOLOv5m</center></td>\n",
        "    <td><center>80.83</center></td>\n",
        "    <td><center>94.35</center></td>\n",
        "    <td><center>80.54</center></td>\n",
        "    <th><center>95 <i>~ 30 FPS</i></center></th>\n",
        "    <th><center>79.33</center></th>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORS = np.array([\n",
        "                    [173, 255, 47],\n",
        "                    [186, 85, 211],\n",
        "                    [255, 215, 0]\n",
        "                  ])\n",
        "\n",
        "def vis(img, boxes, scores, cls_ids, conf=0.5, class_names={0 : \"vehicle\", 1 : \"person\", 2 : \"motorbike\"}):\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        cls_id = int(cls_ids[i])\n",
        "        score = scores[i]\n",
        "        if score < conf:\n",
        "            continue\n",
        "        x0 = int(box[0])\n",
        "        y0 = int(box[1])\n",
        "        x1 = int(box[2])\n",
        "        y1 = int(box[3])\n",
        "\n",
        "        color = (COLORS[cls_id]).astype(np.uint8).tolist()\n",
        "        text = '{} : {:.1f}'.format(class_names[cls_id], score * 100)\n",
        "        txt_color = (0, 0, 0)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        \n",
        "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
        "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
        "\n",
        "        txt_bk_color = (COLORS[cls_id] * 0.7).astype(np.uint8).tolist()\n",
        "        cv2.rectangle(\n",
        "                        img,\n",
        "                        (x0, y0 + 1),\n",
        "                        (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
        "                        txt_bk_color,\n",
        "                        -1\n",
        "                     )\n",
        "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_frame(output, img_info, cls_conf=0.35): # tool che ci fa visualizzare le bboxes!\n",
        "    ratio = img_info[\"ratio\"]\n",
        "    img = img_info[\"raw_img\"]\n",
        "    if output is None: # devo capire come ci si comporta se non ci sono prediction per la singola immagine!!!\n",
        "        return img # questo Ã¨ quando non ci sono predizioni\n",
        "    output = output.cpu()\n",
        "    \n",
        "    bboxes = output[:, 0:4]\n",
        "\n",
        "    # sta roba del artio Ã¨ giÃ  inclusa nel mio postprocessing\n",
        "    #bboxes /= ratio\n",
        "\n",
        "    cls = output[:, 5]\n",
        "    scores = output[:, 4]\n",
        "\n",
        "    vis_res = vis(img, bboxes, scores, cls, cls_conf)\n",
        "    return vis_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_inference(model, img):\n",
        "    img_info = {\"id\": 0}\n",
        "    if isinstance(img, str):\n",
        "        img_info[\"file_name\"] = os.path.basename(img)\n",
        "        img = cv2.imread(img)\n",
        "    else:\n",
        "        img_info[\"file_name\"] = None\n",
        "    \n",
        "    height, width = img.size\n",
        "    img_info[\"height\"] = height\n",
        "    img_info[\"width\"] = width\n",
        "    img_info[\"raw_img\"] = img\n",
        "\n",
        "    ratio = min(model.hparams.img_size / img.size[0], model.hparams.img_size / img.size[1])\n",
        "    img_info[\"ratio\"] = ratio\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "                                    transforms.Resize((model.hparams.img_size, model.hparams.img_size)),\n",
        "                                    transforms.ToTensor()\n",
        "                                   ])\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    img = img.float()\n",
        "    img = img.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        # outputs = postprocess( # Ã¨ il processo per fare le predictions\n",
        "        #     outputs, self.num_classes, self.confthre,\n",
        "        #     self.nmsthre, class_agnostic=True\n",
        "        # )\n",
        "        outputs = torch.tensor([ [[50, 70, 400, 500, 0.99, 0], [60, 70, 300, 200, 0.99, 1], [60, 40, 200, 100, 0.80, 2]] ])\n",
        "    return outputs, img_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_results = True\n",
        "\n",
        "# tutta roba che serve se vogliamo salvare un video\n",
        "cap = cv2.VideoCapture(\"video/Streets_of_Rome.mp4\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "if save_results:\n",
        "    video_writer = cv2.VideoWriter(\"video/results/ris.mp4\", cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\"), 30, (1280, 720)) # fps and dimension of the output video is set\n",
        "    \n",
        "# per ogni frame nel video    \n",
        "while True:\n",
        "    ret_val, frame = cap.read()\n",
        "    if ret_val:\n",
        "        # convert from np.array to PIL Image for inference\n",
        "        img = Image.fromarray(frame)\n",
        "        outputs, img_info = make_inference(model, img)\n",
        "        # but for visualization I don't need PIL Image\n",
        "        img_info[\"raw_img\"] = frame\n",
        "        result_frame = visualize_frame(outputs[0], img_info, model.hparams.conf_threshold)\n",
        "        if save_results:\n",
        "            video_writer.write(frame)\n",
        "        else:\n",
        "            cv2.namedWindow(\"Urbe Perception\", cv2.WINDOW_NORMAL)\n",
        "            cv2.imshow(\"Urbe Perception\", result_frame)\n",
        "            ch = cv2.waitKey(30) # 50 milliseconds per frame --> FPS~20\n",
        "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "                break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "McjXDHbZNROw",
        "FurxaRlU86tq",
        "KgUD3Gw764ei",
        "hXkZIRDeNw_q",
        "eJhhybij_bds"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sappia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "5f06e66338bb5301debc8a4cff3b178f3ee2a0c1aca00670585ce1ed8b6c95da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
