{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckG8WeM-Dgpa"
      },
      "source": [
        "# **URBE *Perception*** ðŸš˜ - *real-time vehicle detection for self-driving cars in Rome*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNF3FMZaI-95"
      },
      "source": [
        "> *Refer to the notebook* [![ðŸ“”](https://colab.reasearch.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sCqnwYm9Dodk1YodD1asVpRMBBdT-8r1#scrollTo=4teaWmm61Fbl) *on **dataset** creation if you haven't already.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAxuP-FDcdvO"
      },
      "source": [
        "In our case for 2D Object detection: we'll detect mostly *vehicles*, *pedestrians* and *motorbikes*. One of the most important task in computer vision and since now it reaches very high accuracy scores, we'll focus on the efficiency/latency part: how much is the inference time of the model? Is it capable of succeding in real-time tasks? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHj5qBhNpaJ"
      },
      "source": [
        "## Imports & Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.hyperparameters import Hparams\n",
        "# from sbert.baseline import SentenceBERT\n",
        "# from sbert.regression_model import execute_booknlp_pipeline\n",
        "# from sbert.regression_model import count_event_sentence\n",
        "# from sbert.regression_model import LengthRegressionModel\n",
        "from src.data_module import URBE_Dataset, URBE_DataModule\n",
        "from src.model import URBE_Perception\n",
        "# from src.train import train_model\n",
        "\n",
        "import dataclasses\n",
        "from dataclasses import asdict\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import pprint\n",
        "import json\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import pytorch_lightning as pl\n",
        "import gc\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import comb\n",
        "import random\n",
        "import json\n",
        "from datasets import load_metric\n",
        "# reproducibility stuff\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "_ = pl.seed_everything(0)\n",
        "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
        "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#%env WANDB_NOTEBOOK_NAME = ./notebook.ipynb\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# login wandb to have the online logger. It is really useful since it stores all the plots and evolution of the model\n",
        "# check also https://docs.wandb.ai/guides/integrations/lightning\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRBCChMTNOqw"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McjXDHbZNROw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ciUHB0NV9a",
        "outputId": "e1a4baf9-8050-4cbe-e455-dbf05ea3c23e"
      },
      "outputs": [],
      "source": [
        "# fast check to see if all the data were correctly imported\n",
        "print(\"We should get the same number in both three cases...\")\n",
        "l1 = os.listdir(\"dataset/URBE_dataset/images/train\") + os.listdir(\"dataset/URBE_dataset/images/test\") + os.listdir(\"dataset/URBE_dataset/images/val\")\n",
        "print(len(l1))\n",
        "\n",
        "d=json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "l2 = list(set([ann[\"image_id\"] for ann in d[\"annotations\"]]))\n",
        "print(len(l2))\n",
        "\n",
        "l3 = d[\"images\"]\n",
        "print(len(l3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "URBE_Data = URBE_DataModule(hparams)\n",
        "# to setup it takes ~6 minutes\n",
        "URBE_Data.setup()\n",
        "print(len(URBE_Data.data_train)) # --> 3500 images\n",
        "print(len(URBE_Data.data_val)) # -->  438 images\n",
        "print(len(URBE_Data.data_test)) # -->  438 images\n",
        "print(\"TOTAL: \"+str(len(URBE_Data.data_train)+len(URBE_Data.data_val)+len(URBE_Data.data_test))+\" images\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FurxaRlU86tq"
      },
      "source": [
        "### Bounding Boxes Visualization\n",
        "\n",
        "It's needed of course for showing the results at the end of the project and during  training of the validation set, but it was essential in the *data processing* phase for understanding the qualities of the datasets' bounding boxes annotations and in general to recognize each different characteristic of the data. <br> *(I tried **Scalabel**, **FiftyOne**, but **WandB** is the best choice)* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Let's test the *dataloaders* and see some samples from a training batch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bbox(label):\n",
        "  ris = { \"predictions\" : {\"box_data\" : [] , \"class_labels\" : {0 : \"vehicle\" , 1 : \"person\", 2 : \"motorbike\"}} }\n",
        "  for ann in label: # for each bbox of the particular image\n",
        "    if ann==[]: # we appended empty lists for having the same batch size for all the samples!\n",
        "      break\n",
        "    position = {\"minX\": ann[0], \"maxX\": ann[0] + ann[2], \"minY\": ann[1], \"maxY\": ann[1] + ann[3]}\n",
        "    class_id = ann[4]\n",
        "    box_caption = ris[\"predictions\"][\"class_labels\"][class_id]\n",
        "    x = {\"position\" : position, \"domain\" : \"pixel\", \"class_id\" : class_id, \"box_caption\" : box_caption}\n",
        "    ris[\"predictions\"][\"box_data\"].append(x)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we take one batch from the training set\n",
        "batch = next(iter(URBE_Data.train_dataloader()))\n",
        "\n",
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"prova\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "transform = T.ToPILImage()\n",
        "images_list = [transform(img) for img in batch[\"img\"]]\n",
        "\n",
        "my_data = []\n",
        "for i,label in enumerate(batch[\"labels\"]):\n",
        "    bbox_list = draw_bbox(label) # label is a list of lists\n",
        "    my_data.append([batch[\"id\"][i], wandb.Image(images_list[i], boxes=bbox_list)])\n",
        "table = wandb.Table(columns=['ID', 'Image'], data=my_data)\n",
        "print(\"logging the table...\")\n",
        "wandb.log({\"dataloaders testing\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkZIRDeNw_q"
      },
      "source": [
        "### Statistics ðŸ“Š"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SerbGxNhg09P"
      },
      "source": [
        "Before starting with the real development of the detection system, we want to plot the statistics of our data. \n",
        "> Since using the dataloaders  for all our dataset is costly and painful, it will be use the \"*annotations.json*\" file as a source for the dataset statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for plotting data --> three group because train/val/test\n",
        "def three_group_bar(columns, data, title, percentage=True): # both columns and data are lists (data is list of a single list)\n",
        "  labels = columns\n",
        "  \n",
        "  train = data[0]\n",
        "  val = data[1]\n",
        "  test = data[2]\n",
        "  \n",
        "  color_list = []\n",
        "  for _ in range(len(data)):\n",
        "    color = [random.randrange(0, 255)/255, random.randrange(0, 255)/255, random.randrange(0, 255)/255, 1]\n",
        "    color_list.append(color)\n",
        "    \n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.15  # the width of the bars\n",
        "  fig, ax = plt.subplots(figsize=(12, 5), layout='constrained')\n",
        "  rects1 = ax.bar(x - width, train, width, label='Train', color=color_list[0])\n",
        "  rects2 = ax.bar(x, val, width, label='Val', color=color_list[1])\n",
        "  rects3 = ax.bar(x + width, test, width, label='Test', color=color_list[2])\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_title(title)\n",
        "  ax.set_xticks(x, labels)\n",
        "  ax.legend()\n",
        "  if percentage:\n",
        "    rects1_labels = [('%.2f' % i) + \"%\" for i in train]\n",
        "    rects2_labels = [('%.2f' % i) + \"%\" for i in val]\n",
        "    rects3_labels = [('%.2f' % i) + \"%\" for i in test]\n",
        "  else:\n",
        "    rects1_labels = train\n",
        "    rects2_labels = val\n",
        "    rects3_labels = test\n",
        "  \n",
        "  ax.bar_label(rects1, rects1_labels, padding=3)\n",
        "  ax.bar_label(rects2, rects2_labels, padding=3)\n",
        "  ax.bar_label(rects3, rects3_labels, padding=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup\n",
        "d = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "annotations = d[\"annotations\"]\n",
        "images = d[\"images\"]\n",
        "\n",
        "train_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/train/\")]\n",
        "val_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/val/\")]\n",
        "test_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/test/\")]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in train_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# VAL\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in val_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# TEST\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in test_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data = [[82.46814899865309, 17.16686171034909, 0.364989290997814], [82.85404948638728, 16.814104764671193, 0.33184574894152646], [82.80439305749428, 16.843565364727365, 0.35204157777836304]]\n",
        "columns = [\"vehicle\", \"person\", \"motorbike\"]\n",
        "three_group_bar(columns, data, \"train/val/test Classes Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2khwW5WH0dqM"
      },
      "source": [
        "**Time of the day**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in train_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# VAL\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in val_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# TEST\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in test_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data = [[57.89522657485811, 35.27379733879222, 6.830976086349678], [58.056361763879785, 34.95927347626627, 6.984364759853946], [58.73741141365162, 34.61395001864976, 6.64863856769862]]\n",
        "columns = [\"day\", \"night\", \"dawn/dusk\"]\n",
        "three_group_bar(columns, data, \"train/val/test TimeOfDay Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhhybij_bds"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6oUMXIyQ5L"
      },
      "source": [
        "We organized the dataset in order to be compatible with the COCO dataset. We did it initially because all the *YOLO* architectures were trained/tested on it.\n",
        "\n",
        "We'll now focus more on the **YOLOv5** model, considered one of the best ones at the moment in terms of the  *accuracy*/*time inference* trade-off and with a very *pytorch* detailed documentation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to achieve the best performances on our custom \"*URBE_dataset*\". In order to realize this we need to perform the following steps:\n",
        "- build a custom YOLOv5 architecture (based on the official repo), to be able to use the *pretrained weights* on the COCO dataset for the **backbone** and the **neck** part. \n",
        "- thanks to the **autoanchor** algorithm implemented by Glenn Jocher (one of the authors of YOLOv5), we compute the best new anchors that fit our dataset. This contributes significantly to enhancing the overall model.\n",
        "- adding the **Mosaic Augmenation** to other basics augmentations techniques to improve the model generalization capability.\n",
        "- trying to attach the **Decoupled Head** at the end of model (as it was added in YOLOv6 and subsequent architectures) and see if there's an improvement.\n",
        "- playing around with different versions of the **IoU loss** (GIoU,DIoU or CIoU)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoanchor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors in YOLO models are predefined bounding boxes used to represent the shape and size of the objects in an image. These anchors are used as a reference to compare the predicted bounding boxes from the model with the actual bounding boxes around the objects. \n",
        "\n",
        "Glenn Jocher introduced the idea of learning anchor boxes based on the distribution of bounding boxes in the custom dataset with *K-means* and *genetic* learning algorithms. This is very important for custom tasks, because the distribution of bounding box sizes and locations may be dramatically different than the preset bounding box anchors in the COCO dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> *The autoanchor algorithm is automatically computed before training (train code made publicly available by the YOLOv5 authors) starts.* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are indeed going to make the annotations (that are in COCO format) compatible with the \"*YOLOv5 text format*\". Then we are going to \"train\" a YOLOv5 architecture on our dataset (but actually we'll only leverage the functionality of autoanchor method). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "annotations = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\", \"r\"))[\"annotations\"]\n",
        "\n",
        "# save image_id for each images in dataset/YOLOv5_format/train and also the .txt name where the labels will be written!\n",
        "image_id_list = []\n",
        "txt_labels_list = []\n",
        "for f in os.listdir(\"dataset/YOLOv5_format/images/1\"):\n",
        "    image_id = f.split(\"_\")[-1][:-4]\n",
        "    txt_labels_name = f[:-4]+\".txt\"\n",
        "    image_id_list.append(image_id)\n",
        "    txt_labels_list.append(txt_labels_name)\n",
        "\n",
        "# filtering of only the annotations of the images in dataset/YOLOv5_format/train\n",
        "filter_annotations = list(filter(lambda x: x[\"image_id\"] in image_id_list, annotations))\n",
        "\n",
        "# for each images in dataset/YOLOv5_format/train\n",
        "for image_id, txt_labels_name in list(zip(image_id_list, txt_labels_list)):\n",
        "    image_labels = list( map(lambda x: [x[\"category_id\"], x[\"bbox\"][0], x[\"bbox\"][1], x[\"bbox\"][2], x[\"bbox\"][3]], list(filter(lambda x: x[\"image_id\"] == image_id, filter_annotations)) ) )\n",
        "    \n",
        "    txt_file_name = \"dataset/YOLOv5_format/labels/\" + txt_labels_name\n",
        "\n",
        "    line_to_write = []\n",
        "    for line in image_labels:\n",
        "        x1 = float(line[1])\n",
        "        y1 = float(line[2])\n",
        "        w = float(line[3])\n",
        "        h = float(line[4])\n",
        "        c1 = round(((x1 + w/2) / 1280), 2)\n",
        "        c2 = round(((y1 + h/2) / 720), 2)\n",
        "        w = round(w/1280, 2)\n",
        "        h = round(h/720, 2)\n",
        "        line_to_write.append(\" \".join([str(line[0]), str(c1), str(c2), str(w), str(h)]))\n",
        "    with open(txt_file_name, 'w') as f:\n",
        "        f.write(\"\\n\".join(line_to_write))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> After downloading the YOLOv5 format dataset on *Roboflow*, we \"train\" it using the YOLOv5 official code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#user_name = \"lavallone\"\n",
        "#project_name = \"VISIOPE_project\"\n",
        "#version_name = \"prova\"\n",
        "#run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "data = URBE_DataModule(hparams)\n",
        "model = URBE_Perception(hparams)\n",
        "#trainer = train_model(data, model, experiment_name = version_name, \\\n",
        "#    patience=5, metric_to_monitor=\"val_ROUGE\", mode=\"max\", epochs = 10)\n",
        "\n",
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.setup()\n",
        "model.to(\"cuda\")\n",
        "for batch in iter(data.train_dataloader()):\n",
        "    with torch.no_grad():\n",
        "        batch[\"img\"] = batch[\"img\"].to(\"cuda\")\n",
        "        #print(batch[\"img\"].shape)\n",
        "        x = model(batch[\"img\"])\n",
        "        print(x[0].shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s=[[2,3],[3,4],[5,4]]\n",
        "x=torch.tensor([[ [[2,3],[3,4]], [[2,3],[3,4]], [[2,3],[3,4]] ],\n",
        "               [ [[2,3],[3,4]], [[2,3],[3,4]], [[2,3],[3,4]] ] ]) \n",
        "x.shape\n",
        "\n",
        "x[..., ::2, ::2]\n",
        "#(b,c,w,h) -> y(b,4c,w/2,h/2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(\"Screenshot from 2022-12-19 18-32-25.png\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "print(img)\n",
        "resized_img = cv2.resize(\n",
        "            img,\n",
        "            (int(img.shape[1] * 1), int(img.shape[0] * 1)),\n",
        "            interpolation=cv2.INTER_LINEAR,\n",
        "        ).astype(np.uint8)\n",
        "print(resized_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "#model = model.model.model.model[:10]\n",
        "model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "McjXDHbZNROw",
        "FurxaRlU86tq",
        "KgUD3Gw764ei",
        "hXkZIRDeNw_q",
        "eJhhybij_bds"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sappia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "5f06e66338bb5301debc8a4cff3b178f3ee2a0c1aca00670585ce1ed8b6c95da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
