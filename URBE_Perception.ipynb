{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckG8WeM-Dgpa"
      },
      "source": [
        "# **URBE *Perception*** ðŸš˜ - *real-time vehicle detection for self-driving cars in Rome*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CNF3FMZaI-95"
      },
      "source": [
        "> *Refer to the notebook* [ðŸ“™](https://colab.research.google.com/drive/1sCqnwYm9Dodk1YodD1asVpRMBBdT-8r1?usp=share_link) *on **dataset** creation if you haven't already and to the one were official YOLOv5 models are **finetuned** on my custom dataset* [ðŸ“—](https://colab.research.google.com/drive/1Kb_M6O7NMIdwVFk4EDGM-1NH7fwyMK1p?usp=share_link)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why edge computing and self-driving cars?**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The significance of edge computing in self-driving cars lies in its ability to process data and make decisions in real-time at the edge of the network, near the source of the data. This is imperative for self-driving cars, as they require prompt and precise decision-making based on information obtained from sensors such as cameras, radar, and lidar.\n",
        "\n",
        "In a cloud computing software architecture, data is sent to a central server for processing and then the results are sent back to the device. This approach doesn't work for self-driving cars, as the latency, or delay, of transmitting data back and forth between the car and a central server can be dangerous in a real-time driving scenario. With edge computing, the data is processed locally on the car, reducing latency and improving response times. It is indeed a key technology for enabling self-driving cars to operate safely and effectively.\n",
        "\n",
        "The popularity of this approach to constructing intelligent systems is growing, especially in light of a potentially challenging future in terms of the scarcity of materials for performing extensive computations. For this reason many researchers in the AI field are pursuing this direction (https://news.mit.edu/2023/autonomous-vehicles-carbon-emissions-0113)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The idea**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it can be imagined, the computational system behind a self-driving car is huge and extremely complex; it integrates many technologies, including sensing (lidars, cameras, radars), localization, decision making and **perception** on which my work is focused. <br> **Urbe** stands for \"*city*\" and it is used to be referred to the city of Rome. In fact my final goal is to build a real-time system which runs on embedded devices (as *Nvidia Jetson Nano* or *Google Coral* ) and which detects *vehicles*, *pedestrians* and *motorbikes* on the streets of Rome. In effect, a real submodule for autonomous cars. But for this project I only built an object detection system with an eye toward  the  inference time and most importantly towards the application scenario, Rome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHj5qBhNpaJ"
      },
      "source": [
        "## Imports & Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install the requirements\n",
        "%pip install -r requirements.txt > /dev/null\n",
        "# set to false if you already have the dataset\n",
        "download_dataset = False \n",
        "if download_dataset:\n",
        "    %cd dataset\n",
        "    !bash download_dataset.sh\n",
        "    %cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.hyperparameters import Hparams\n",
        "from src.data_module import URBE_DataModule\n",
        "from src.model import URBE_Perception\n",
        "from src.loss import YOLO_Loss\n",
        "from src.train import train_model\n",
        "\n",
        "from dataclasses import asdict\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import json\n",
        "import torchvision.transforms as T\n",
        "import pytorch_lightning as pl\n",
        "import gc\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import cv2\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# reproducibility stuff\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "_ = pl.seed_everything(0)\n",
        "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
        "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#%env WANDB_NOTEBOOK_NAME = ./notebook.ipynb\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# login wandb to have the online logger. It is really useful since it stores all the plots and evolution of the model\n",
        "# check also https://docs.wandb.ai/guides/integrations/lightning\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McjXDHbZNROw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "URBE_Data = URBE_DataModule(hparams)\n",
        "URBE_Data.setup()\n",
        "print(len(URBE_Data.data_train))\n",
        "print(len(URBE_Data.data_val))\n",
        "print(len(URBE_Data.data_test))\n",
        "print(\"TOTAL: \"+str(len(URBE_Data.data_train)+len(URBE_Data.data_val)+len(URBE_Data.data_test))+\" images\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FurxaRlU86tq"
      },
      "source": [
        "### Bounding Boxes Visualization\n",
        "\n",
        "It's needed of course for showing the results at the end of the project and during  training of the validation set, but it was essential in the *data processing* phase for understanding the qualities of the datasets' bounding boxes annotations and in general to recognize each different characteristic of the data. <br> *(I tried **Scalabel**, **FiftyOne**, but **WandB** is the best choice)* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Let's test the *dataloaders* and see some samples from a training batch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bbox(label):\n",
        "  ris = { \"predictions\" : {\"box_data\" : [] , \"class_labels\" : {0 : \"vehicle\" , 1 : \"person\", 2 : \"motorbike\"}} }\n",
        "  for ann in label: # for each bbox of the particular image\n",
        "    if ann.sum()==0: # we appended this [0,0,0,0,0] type of list for having the same batch size for all the samples!\n",
        "      break\n",
        "    position = {\"minX\": ((ann[1]-(ann[3]/2))*1280).item(), \"maxX\": ((ann[1]+(ann[3]/2))*1280).item(), \"minY\": ((ann[2]-(ann[4]/2))*720).item(), \"maxY\": ((ann[2]+(ann[4]/2))*720).item()}\n",
        "    class_id = int(ann[0])\n",
        "    box_caption = ris[\"predictions\"][\"class_labels\"][class_id]\n",
        "    x = {\"position\" : position, \"domain\" : \"pixel\", \"class_id\" : class_id, \"box_caption\" : box_caption}\n",
        "    ris[\"predictions\"][\"box_data\"].append(x)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we take one batch from the training set\n",
        "batch = next(iter(URBE_Data.train_dataloader()))\n",
        "\n",
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"dataset\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "transform = T.ToPILImage()\n",
        "images_list = [transform(img) for img in batch[\"img\"]]\n",
        "images_list = [img.resize((1280, 720)) for img in images_list]\n",
        "\n",
        "my_data = []\n",
        "for i,label in enumerate(batch[\"labels\"]):\n",
        "    bbox_list = draw_bbox(label) # label is a list of lists\n",
        "    my_data.append([batch[\"id\"][i], wandb.Image(images_list[i], boxes=bbox_list)])\n",
        "table = wandb.Table(columns=['ID', 'Image'], data=my_data)\n",
        "print(\"logging the table...\")\n",
        "wandb.log({\"dataloaders testing\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkZIRDeNw_q"
      },
      "source": [
        "### Statistics ðŸ“Š"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SerbGxNhg09P"
      },
      "source": [
        "Before starting with the real development of the detection system, we want to plot the statistics of our data. \n",
        "> Since using the dataloaders  for all our dataset is costly and painful, it will be use the \"*annotations.json*\" file as a source for the dataset statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for plotting data --> three group because train/val/test\n",
        "def three_group_bar(columns, data, title, percentage=True): # both columns and data are lists (data is list of a single list)\n",
        "  labels = columns\n",
        "  \n",
        "  train = data[0]\n",
        "  val = data[1]\n",
        "  test = data[2]\n",
        "  \n",
        "  color_list = []\n",
        "  for _ in range(len(data)):\n",
        "    color = [random.randrange(0, 255)/255, random.randrange(0, 255)/255, random.randrange(0, 255)/255, 1]\n",
        "    color_list.append(color)\n",
        "    \n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.15  # the width of the bars\n",
        "  fig, ax = plt.subplots(figsize=(12, 5), layout='constrained')\n",
        "  rects1 = ax.bar(x - width, train, width, label='Train', color=color_list[0])\n",
        "  rects2 = ax.bar(x, val, width, label='Val', color=color_list[1])\n",
        "  rects3 = ax.bar(x + width, test, width, label='Test', color=color_list[2])\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_title(title)\n",
        "  ax.set_xticks(x, labels)\n",
        "  ax.legend()\n",
        "  if percentage:\n",
        "    rects1_labels = [('%.2f' % i) + \"%\" for i in train]\n",
        "    rects2_labels = [('%.2f' % i) + \"%\" for i in val]\n",
        "    rects3_labels = [('%.2f' % i) + \"%\" for i in test]\n",
        "  else:\n",
        "    rects1_labels = train\n",
        "    rects2_labels = val\n",
        "    rects3_labels = test\n",
        "  \n",
        "  ax.bar_label(rects1, rects1_labels, padding=3)\n",
        "  ax.bar_label(rects2, rects2_labels, padding=3)\n",
        "  ax.bar_label(rects3, rects3_labels, padding=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup\n",
        "d = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\"))\n",
        "annotations = d[\"annotations\"]\n",
        "images = d[\"images\"]\n",
        "\n",
        "train_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/train/\")]\n",
        "val_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/val/\")]\n",
        "test_image_id_list = [f.split(\"_\")[-1][:-4] for f in os.listdir(\"dataset/URBE_dataset/images/test/\")]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in train_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# VAL\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in val_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])\n",
        "\n",
        "# TEST\n",
        "classes_list = [ann[\"category_id\"] for ann in tqdm(annotations) if ann[\"image_id\"] in test_image_id_list]\n",
        "c = Counter(classes_list)\n",
        "tot = c[0] + c[1] + c[2]\n",
        "data.append([(c[0]/tot)*100, (c[1]/tot)*100, (c[2]/tot)*100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[82.46814899865309, 17.16686171034909, 0.364989290997814], [82.85404948638728, 16.814104764671193, 0.33184574894152646], [82.80439305749428, 16.843565364727365, 0.35204157777836304]]\n",
        "columns = [\"vehicle\", \"person\", \"motorbike\"]\n",
        "three_group_bar(columns, data, \"train/val/test Classes Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2khwW5WH0dqM"
      },
      "source": [
        "**Time of the day**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in train_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# VAL\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in val_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])\n",
        "\n",
        "# TEST\n",
        "time_list = [img[\"timeofday\"] for img in tqdm(images) if img[\"id\"] in test_image_id_list]\n",
        "c = Counter(time_list)\n",
        "tot = c[\"daytime\"] + c[\"Day\"] + c[\"night\"] + c[\"Night\"] + c[\"dawn/dusk\"] + c[\"Dawn/Dusk\"]\n",
        "data.append([ ((c[\"daytime\"]+c[\"Day\"])/tot)*100, ((c[\"night\"]+c[\"Night\"])/tot)*100, ((c[\"dawn/dusk\"]+c[\"Dawn/Dusk\"])/tot)*100 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[57.89522657485811, 35.27379733879222, 6.830976086349678], [58.056361763879785, 34.95927347626627, 6.984364759853946], [58.73741141365162, 34.61395001864976, 6.64863856769862]]\n",
        "columns = [\"day\", \"night\", \"dawn/dusk\"]\n",
        "three_group_bar(columns, data, \"train/val/test TimeOfDay Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhhybij_bds"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6oUMXIyQ5L"
      },
      "source": [
        "We organized the dataset in order to be compatible with the COCO dataset. We did it initially because all the *YOLO* architectures were trained/tested on it.\n",
        "\n",
        "We'll now focus more on the **YOLOv5** model, considered one of the best ones at the moment in terms of the  *accuracy*/*time inference* trade-off and with a very *pytorch-detailed* documentation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to achieve the best performances on our custom \"*URBE_dataset*\". In order to realize this we need to perform the following steps:\n",
        "- build a custom YOLOv5 architecture (based on the official repo), to be able to use the *pretrained weights* on the COCO dataset for the **backbone** and the **neck** part. \n",
        "- thanks to the **autoanchor** algorithm implemented by Glenn Jocher (one of the authors of YOLOv5), we compute the best new anchors that fit our dataset. This contributes significantly to enhancing the overall model.\n",
        "- adding only basic **augmentations** on the images and also on the bounding boxes thanks to ***Albumentation*** library. We decided to not apply *Mosaic Augmentation* (that is one of main suggested augmentation techniques for YOLOv5) because the custom dataset already conveys to the model a big enough generalization capability.\n",
        "- trying to attach the **Decoupled Head** at the end of model (as it was added in YOLOv6 and subsequent architectures) and see if there's an improvement.\n",
        "- playing around with different versions of the **IoU loss** (GIoU, DIoU or CIoU)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The *git repos* from which I took some informations about building the model architecture are:\n",
        "- https://github.com/ultralytics/yolov5\n",
        "- https://github.com/AlessandroMondin/YOLOV5m\n",
        "- https://github.com/Iywie/pl_YOLO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoanchor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors in YOLO models are predefined bounding boxes used to represent the shape and size of the objects in an image. These anchors are used as a reference to compare the predicted bounding boxes from the model with the actual bounding boxes around the objects. \n",
        "\n",
        "Glenn Jocher introduced the idea of learning anchor boxes based on the distribution of bounding boxes in the custom dataset with *K-means* and *genetic* learning algorithms. This is very important for custom tasks, because the distribution of bounding box sizes and locations may be dramatically different than the preset bounding box anchors in the COCO dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> *The autoanchor algorithm is automatically computed before training (train code made publicly available by the YOLOv5 authors) starts.* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are indeed going to make the annotations (that are in COCO format) compatible with the \"*YOLOv5 text format*\". Then we are going to \"train\" a YOLOv5 architecture on our dataset (but actually we'll only leverage the functionality of autoanchor method). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "annotations = json.load(open(\"dataset/URBE_dataset/labels/COCO/annotations.json\", \"r\"))[\"annotations\"]\n",
        "\n",
        "# save image_id for each images in dataset/YOLOv5_format/train and also the .txt name where the labels will be written!\n",
        "image_id_list = []\n",
        "txt_labels_list = []\n",
        "for f in os.listdir(\"dataset/YOLOv5_format/images/1\")+os.listdir(\"dataset/YOLOv5_format/images/2\")+os.listdir(\"dataset/YOLOv5_format/images/3\")+os.listdir(\"dataset/YOLOv5_format/images/4\"):\n",
        "    image_id = f.split(\"_\")[-1][:-4]\n",
        "    txt_labels_name = f[:-4]+\".txt\"\n",
        "    image_id_list.append(image_id)\n",
        "    txt_labels_list.append(txt_labels_name)\n",
        "\n",
        "# filtering of only the annotations of the images in dataset/YOLOv5_format/train\n",
        "filter_annotations = list(filter(lambda x: x[\"image_id\"] in image_id_list, annotations))\n",
        "\n",
        "# for each images in dataset/YOLOv5_format/train\n",
        "for image_id, txt_labels_name in list(zip(image_id_list, txt_labels_list)):\n",
        "    image_labels = list( map(lambda x: [x[\"category_id\"], x[\"bbox\"][0], x[\"bbox\"][1], x[\"bbox\"][2], x[\"bbox\"][3]], list(filter(lambda x: x[\"image_id\"] == image_id, filter_annotations)) ) )\n",
        "    \n",
        "    txt_file_name = \"dataset/YOLOv5_format/labels/\" + txt_labels_name\n",
        "\n",
        "    line_to_write = []\n",
        "    for line in image_labels:\n",
        "        x1 = float(line[1])\n",
        "        y1 = float(line[2])\n",
        "        w = float(line[3])\n",
        "        h = float(line[4])\n",
        "        c1 = round(((x1 + w/2) / 1280), 2)\n",
        "        c2 = round(((y1 + h/2) / 720), 2)\n",
        "        w = round(w/1280, 2)\n",
        "        h = round(h/720, 2)\n",
        "        line_to_write.append(\" \".join([str(line[0]), str(c1), str(c2), str(w), str(h)]))\n",
        "    with open(txt_file_name, 'w') as f:\n",
        "        f.write(\"\\n\".join(line_to_write))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> <a href=\"https://imgur.com/Iebkt2Y\"><img src=\"https://i.imgur.com/Iebkt2Y.png\" width=65 height=25 title=\"source: imgur.com\" /></a> After downloading the YOLOv5 format dataset on *Roboflow*, we \"train\" it using the YOLOv5 official code."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We followed the [colab Roboflow tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb) about training YOLOv5 with a dataset already uploaded in Roboflow in order to perform the autoanchor algorithm and see if the default given anchors fit the dataset. The answer was positive and therefore we didn't have to change them. \n",
        "\n",
        "<a href=\"https://imgur.com/zy6z9o9\"><img src=\"https://i.imgur.com/zy6z9o9.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretrained weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aim is to load the *pretrained* weights of YOLOv5 architecture in our models. We create four \"*.pt*\" files because we have to deal with two versions of the YOLOv5 model (**medium** and **small**) and with two different types of HEADs (**Simple** and **Decoupled**). For this reason, we're going to load only the pretrained weights of the BACKBONE and the NECK. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../yolov5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5m**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5m - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5m.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5m_nh_decoupled.pt\")\n",
        "model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOLOv5n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Simple HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-7]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_simple.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv5n - Decoupled HEAD\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "my_weights = model.state_dict()\n",
        "pretrained_weights = torch.load(\"pretrained/ultralytics_yolov5n.pt\")[\"model\"].state_dict()\n",
        "\n",
        "# manually loading ultralytics weights in my architecture\n",
        "state_dict = model.state_dict()\n",
        "layers_loaded = []\n",
        "for layer, weight in list(pretrained_weights.items())[:-7]:\n",
        "    for my_layer, my_weight in list(state_dict.items())[:-109]:\n",
        "        if weight.shape == my_weight.shape:\n",
        "            if my_layer not in layers_loaded:\n",
        "                state_dict[my_layer] = weight\n",
        "                layers_loaded.append(my_layer)\n",
        "                break\n",
        "\n",
        "torch.save(state_dict, \"pretrained/yolov5n_nh_decoupled.pt\")\n",
        "#model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_name = \"lavallone\"\n",
        "project_name = \"VISIOPE_project\"\n",
        "version_name = \"yolov5n_decoupled\"\n",
        "run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n",
        "\n",
        "hparams = asdict(Hparams())\n",
        "data = URBE_DataModule(hparams)\n",
        "model = URBE_Perception(hparams)\n",
        "\n",
        "if hparams[\"load_pretrained\"]:\n",
        "    if hparams[\"first_out\"] == 48:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5m_nh_decoupled.pt\"))\n",
        "    else:\n",
        "        if hparams[\"head\"] == \"simple\":\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_simple.pt\"))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(\"pretrained/yolov5n_nh_decoupled.pt\"))\n",
        "            \n",
        "# RESUME logic is embedded within the trainer\n",
        "trainer = train_model(data, model, experiment_name = version_name, \\\n",
        "   patience=15, metric_to_monitor=\"map_50\", mode=\"max\", epochs = 100)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ultimate objective is to create a *real-time detection model* that can be integrated into the intricate self-driving system software architecture. In addition to performance metrics, the inference time of the model is also crucial, perhaps even more so."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategies for decrease Inference Time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mainly leverage three methods:\n",
        "\n",
        "- **Floating Point 16 precision (FP16)**. Lowering weights network precision, such as the 16-bit floating-point, enables the model to process inputs faster. This capability can also be levareaged during training enabling the deployment of large neural networks since they require less memory and they run faster (achieving upto +3X speedups on modern GPUs).\n",
        "- **Pruning**. We use the *l1_unstructured* pruning method which acts by zeroing out the units with the lowest L1-norm. We simply apply it after the training phase (*post-pruning method*) by applying *0.3* and *0.5* sparsity amounts.\n",
        "- **Quantization**. Model quantization is another performance optimization technique that allows speeding up inference and decreasing memory requirements by performing computations and storing tensors at lower bitwidths than floating-point precision. This is particularly beneficial during model deployment. We use *Quantization Aware Training (QAT)*, which mimics the effects of quantization during training: the computations are carried-out in floating-point precision but the subsequent quantization effect is taken into account. The weights and activations are quantized into lower precision only for inference, when training is completed."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">  My idea was to apply **QAT** on my custom trained models because was easily applicable by adding the *QuantizationAwareTraining()* pytorch-lightning built-in callbacks to the pl.Trainer class. But due to an internal problem of the callback (see https://github.com/Lightning-AI/lightning/issues/16609) it was not possible to do it.\n",
        "\n",
        "<a href=\"https://imgur.com/VtrnS2h\"><img src=\"https://i.imgur.com/VtrnS2h.png\" width=750 height=80 title=\"source: imgur.com\" /></a>\n",
        "\n",
        "> For this reason, both for my *custom* trained models and for the official *finetuned* YOLOv5 models on my *URBE dataset*, only **fp16** and **post-pruning** techinques are used to reduce the inference time."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to compute *inference* time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input\n",
        "hparams = asdict(Hparams())\n",
        "hparams[\"max_number_images\"] = 16\n",
        "hparams[\"batch_size\"] = 1\n",
        "URBE_Data = URBE_DataModule(hparams)\n",
        "URBE_Data.setup()\n",
        "batch = next(iter(URBE_Data.train_dataloader()))\n",
        "device = torch.device(\"cuda\")\n",
        "img_input = batch[\"img\"].to(device)\n",
        "\n",
        "# model\n",
        "finetuned = False\n",
        "if finetuned:\n",
        "   model = torch.hub.load('ultralytics/yolov5', 'yolov5n')\n",
        "else:\n",
        "   hparams = asdict(Hparams())\n",
        "   model = URBE_Perception(hparams)\n",
        "   model_ckpt = \"models/yolov5n_decoupled-epoch=47-map_50=0.2797.ckpt\"\n",
        "   model = URBE_Perception.load_from_checkpoint(model_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "fp16 = False\n",
        "if fp16:\n",
        "   model = model.half()\n",
        "   img_input = img_input.half()\n",
        "   \n",
        "prune = False\n",
        "amount = 0.3\n",
        "if prune:\n",
        "   import torch.nn.utils.prune as prune\n",
        "   for name, m in model.named_modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "         prune.l1_unstructured(m, name='weight', amount=amount)\n",
        "         prune.remove(m, 'weight')\n",
        "\n",
        "# how to correctly compute inference time (therefore fps) for a model\n",
        "# https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "repetitions = 300\n",
        "timings = np.zeros((repetitions,1))\n",
        "\n",
        "# GPU-WARM-UP\n",
        "for _ in range(10):\n",
        "   _ = model(img_input)\n",
        "   \n",
        "# MEASURE PERFORMANCE\n",
        "with torch.no_grad():\n",
        "  for rep in range(repetitions):\n",
        "     starter.record()\n",
        "     _ = model(img_input)\n",
        "     ender.record()\n",
        "     # WAIT FOR GPU SYNC\n",
        "     torch.cuda.synchronize()\n",
        "     curr_time = starter.elapsed_time(ender)\n",
        "     timings[rep] = curr_time\n",
        "\n",
        "mean_syn = np.sum(timings) / repetitions\n",
        "inference_time = mean_syn\n",
        "fps = 1 / (inference_time/1000)\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Parameters: {num_param}\")\n",
        "print(f\"Inference time: {inference_time:.3f} ms\")\n",
        "print(f\"Frame Per Second: {fps:.3f}\")\n",
        "print(\"---------------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorRT <a href=\"https://imgur.com/Unhn3jE\"><img width=55 height= 25 src=\"https://i.imgur.com/Unhn3jE.png\" title=\"source: imgur.com\" /></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorRT is a deep learning inference optimizer and runtime library developed by NVIDIA. It is designed to optimize and accelerate the inference of deep learning models on NVIDIA GPUs, making it suitable for deployment in various embedded environments. Considering our application for self-driving cars and the need for real-time inference, this is a step we cannot do without.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We tried to convert our trained models to TensorRT format and run inference on them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "!python setup.py install\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../torch2trt\n",
        "import torch\n",
        "from torch2trt import torch2trt\n",
        "\n",
        "model.to(\"cuda\")\n",
        "# create example data\n",
        "x = torch.zeros((1, 3, 640, 640)).cuda()\n",
        "\n",
        "# convert to TensorRT feeding sample data as input\n",
        "model_trt = torch2trt(model, [x]) # we can now execute the returned TRTModule just like the original PyTorch model :)\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE \n",
        "torch.save(model_trt.state_dict(), 'model_trt.pth')\n",
        "\n",
        "# and LOAD\n",
        "%cd ../torch2trt\n",
        "from torch2trt import TRTModule\n",
        "model_trt = TRTModule()\n",
        "model_trt.load_state_dict(torch.load('model.pth'))\n",
        "%cd ../VISIOPE_project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_performance(model, data, device, fp16):\n",
        "    model.eval()\n",
        "    dataset = data.test_dataloader() # TEST SET\n",
        "    \n",
        "    mAP = MeanAveragePrecision() # in this way the IoU thresholds are taken from the stepped range [0.5,...,0.95] with step 0.05\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mAP_list = []\n",
        "        for i, batch in enumerate(tqdm(iter(dataset))): # tqdm let us to visualize dataset reading process\n",
        "            imgs = batch[\"img\"]\n",
        "            imgs = imgs.to(device)\n",
        "            if fp16:\n",
        "                imgs = imgs.half()\n",
        "            out = model(imgs)\n",
        "            \n",
        "            targets = [YOLO_Loss.transform_targets(out, bboxes, torch.tensor(URBE_Perception.ANCHORS), URBE_Perception.STRIDE) for bboxes in batch[\"labels\"]]\n",
        "            # I want targets to be the same shape as predictions --> (bs, 3 , 80/40/20, 80/40/20, 6)\n",
        "            t1 = torch.stack([target[0] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t2 = torch.stack([target[1] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            t3 = torch.stack([target[2] for target in targets], dim=0).to(device,non_blocking=True)\n",
        "            targets = [t1, t2, t3]\n",
        "            \n",
        "            pred_boxes = model.cells_to_bboxes(out, model.head.anchors, model.head.stride, device, is_pred=True)\n",
        "            true_boxes = model.cells_to_bboxes(targets, model.head.anchors, model.head.stride, device, is_pred=False)\n",
        "            _, _, pred_boxes = model.non_max_suppression(pred_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, max_detections=50, is_pred=True, filenames=batch[\"file_name\"])\n",
        "            true_boxes = model.non_max_suppression(true_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, max_detections=50, is_pred=False)\n",
        "            \n",
        "            pred_dict_list = []\n",
        "            for b in range(len(pred_boxes)):\n",
        "                if pred_boxes[b].numel() == 0: # if the model hasn't predict any bboxes\n",
        "                    pred_dict_list.append( dict(boxes=torch.tensor([]).to(device), scores=torch.tensor([]).to(device), labels=torch.tensor([]).to(device),) )\n",
        "                else:\n",
        "                    pred_dict_list.append( dict(boxes=pred_boxes[b][..., 2:], scores=pred_boxes[b][..., 1], labels=pred_boxes[b][..., 0],) )\n",
        "            true_dict_list = [ dict(boxes=true_boxes[i][..., 2:], labels=true_boxes[i][..., 0],) for i in range(len(true_boxes)) ]\n",
        "            \n",
        "            mAP.update(pred_dict_list, true_dict_list)\n",
        "            \n",
        "            mAP_50_95 = mAP.compute()[\"map\"]\n",
        "            mAP_50 = mAP.compute()[\"map_50\"]\n",
        "            \n",
        "            print(f\"Batch: {i+1}\")\n",
        "            print(f\"mAP_50_95: {mAP_50_95:.3f}\")\n",
        "            print(f\"mAP_50: {mAP_50:.3f}\")\n",
        "            mAP_list.append((mAP_50_95, mAP_50))\n",
        "        \n",
        "        print()\n",
        "        all_map_50 = [e[1] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@0.5: {np.array(all_map_50).mean()}\")\n",
        "        print()\n",
        "        all_map_50_95 = [e[0] for e in mAP_list]\n",
        "        print(f\"AVERAGE TEST SET mAP@[0.5:0.95]: {np.array(all_map_50_95).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define which model we want to test\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "model_ckpt = \"models/yolov5n_decoupled-epoch=47-map_50=0.2797.ckpt\"\n",
        "model = URBE_Perception.load_from_checkpoint(model_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "fp16 = True\n",
        "if fp16:\n",
        "   model = model.half()\n",
        "   \n",
        "prune = True\n",
        "amount = 0.5\n",
        "if prune:\n",
        "   import torch.nn.utils.prune as prune\n",
        "   for name, m in model.named_modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "         prune.l1_unstructured(m, name='weight', amount=amount)\n",
        "         prune.remove(m, 'weight')\n",
        "\n",
        "# if we want to test without training before we need to setup the data\n",
        "trained = True\n",
        "if not trained:\n",
        "    hparams = asdict(Hparams())\n",
        "    data = URBE_DataModule(hparams)\n",
        "    data.setup()\n",
        "\n",
        "evaluate_performance(model, data, device, fp16)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define which model we want to test\n",
        "hparams = asdict(Hparams())\n",
        "model = URBE_Perception(hparams)\n",
        "best_ckpt = \"models/yolov5m_decoupled-epoch=27-map_50=0.4074.ckpt\"\n",
        "model = URBE_Perception.load_from_checkpoint(best_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.hparams.nms_iou_thresh = 0.15\n",
        "model.hparams.conf_threshold = 0.65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORS = np.array([\n",
        "                    [173, 255, 47],\n",
        "                    [186, 85, 211],\n",
        "                    [255, 215, 0]\n",
        "                  ])\n",
        "\n",
        "def vis(img, boxes, scores, cls_ids, class_names={0 : \"vehicle\", 1 : \"person\", 2 : \"motorbike\"}):\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        cls_id = int(cls_ids[i])\n",
        "        score = scores[i]\n",
        "        \n",
        "        x0 = int(box[0]*(1280/640))\n",
        "        y0 = int(box[1]*(720/640))\n",
        "        x1 = int(box[2]*(1280/640))\n",
        "        y1 = int(box[3]*(720/640))\n",
        "\n",
        "        color = (COLORS[cls_id]).astype(np.uint8).tolist()\n",
        "        text = '{} : {:.1f}'.format(class_names[cls_id], score * 100)\n",
        "        txt_color = (0, 0, 0)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        \n",
        "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
        "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
        "\n",
        "        txt_bk_color = (COLORS[cls_id] * 0.7).astype(np.uint8).tolist()\n",
        "        cv2.rectangle(\n",
        "                        img,\n",
        "                        (x0, y0 + 1),\n",
        "                        (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
        "                        txt_bk_color,\n",
        "                        -1\n",
        "                     )\n",
        "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_frame(output, img_info):\n",
        "    img = img_info[\"raw_img\"]\n",
        "    if output is None: # no predictions for the frame\n",
        "        return img\n",
        "    \n",
        "    output = output.cpu()\n",
        "    bboxes = output[:, 0:4]\n",
        "    cls = output[:, 5].int()\n",
        "    scores = output[:, 4].float()\n",
        "\n",
        "    vis_res = vis(img, bboxes, scores, cls)\n",
        "    return vis_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_inference(model, img):\n",
        "    img_info = {}\n",
        "    if isinstance(img, str):\n",
        "        img_info[\"file_name\"] = os.path.basename(img)\n",
        "        img = cv2.imread(img)\n",
        "    else:\n",
        "        img_info[\"file_name\"] = None\n",
        "    \n",
        "    height, width = img.size\n",
        "    img_info[\"height\"] = height # 720\n",
        "    img_info[\"width\"] = width # 1280\n",
        "    img_info[\"raw_img\"] = img # PIL Image\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "                                    transforms.Resize((model.hparams.img_size, model.hparams.img_size)),\n",
        "                                    transforms.ToTensor()\n",
        "                                   ])\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    img = img.float()\n",
        "    img = img.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #### FORWARD PHASE ####\n",
        "        outputs = model(img)\n",
        "        pred_boxes = model.cells_to_bboxes(outputs, model.head.anchors, model.head.stride, model.device, is_pred=True)\n",
        "        _, _, pred_boxes = model.non_max_suppression(pred_boxes, iou_threshold=model.hparams.nms_iou_thresh, threshold=model.hparams.conf_threshold, max_detections=20, is_pred=True, filenames=[\"frame\"])\n",
        "        \n",
        "        if pred_boxes[0].numel() == 0: # if the model hasn't predict any bboxes\n",
        "            outputs = [None]\n",
        "        else:\n",
        "            outputs = torch.cat((pred_boxes[0][..., 2:], pred_boxes[0][..., 1:2], pred_boxes[0][..., 0:1],), dim=-1).unsqueeze(0)\n",
        "        #### ------------- ####\n",
        "    return outputs, img_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_results = True\n",
        "cap = cv2.VideoCapture(\"video/Streets_of_Rome.mp4\")\n",
        "\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "if save_results:\n",
        "    video_writer = cv2.VideoWriter(\"video/ris.mp4\", cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\"), 30, (1280, 720)) # fps and dimension of the output video is set\n",
        "# for each video frame   \n",
        "while True:\n",
        "    ret_val, frame = cap.read()\n",
        "    if ret_val:\n",
        "        # convert from np.array to PIL Image for inference\n",
        "        img = Image.fromarray(frame) # PIL Image\n",
        "        outputs, img_info = make_inference(model, img)\n",
        "        # but for visualization I don't need PIL Image\n",
        "        img_info[\"raw_img\"] = frame # np.array\n",
        "        result_frame = visualize_frame(outputs[0], img_info)\n",
        "        if save_results:\n",
        "            video_writer.write(frame)\n",
        "        else:\n",
        "            cv2.namedWindow(\"Urbe Perception\", cv2.WINDOW_NORMAL)\n",
        "            cv2.imshow(\"Urbe Perception\", result_frame)\n",
        "            ch = cv2.waitKey(30) # 50 milliseconds per frame --> FPS~20\n",
        "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
        "                break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "if save_results:\n",
        "    video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Custom models**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " <table>\n",
        "  <tr>\n",
        "    <th><center> Model </center></th>\n",
        "    <th><center>  </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50</t> </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50-95</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b1</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b16</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> params <br><t style=\"font-size:12px;\">(M)</t> </center></th>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_m</b><br>simple head</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center>0.416<br><b>0.417</b><br>0.401<br>0.305</center></td>\n",
        "    <td><center><b>0.206</b><br>0.205<br>0.197<br>0.150</center></td>\n",
        "    <td><center>17.448 <i>~ 57 FPS<br><b>17.081 <i>~ 58 FPS</b><br>17.211 <i>~ 58 FPS<br>16.909 <i>~ 59 FPS</center></td>\n",
        "    <td><center>264.478<br><b>194.542</b><br>197.870<br>195.424</td>\n",
        "    <td><center><t style=\"font-size:18px;\">20.87</t></center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_m</b><br>decoupled head</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.473</b><br>0.472<br>0.448<br>0.347</center></td>\n",
        "    <td><center>0.248<br><b>0.251</b><br>0.230<br>0.149</center></td>\n",
        "    <td><center>27.580 <i>~ 36 FPS<br><b>26.122 <i>~ 38 FPS</b><br>26.772 <i>~ 37 FPS<br>26.323 <i>~ 38 FPS</center></td>\n",
        "    <td><center>404.726<br>290.056<br><b>268.198</b><br>293.334</td>\n",
        "    <td><center><t style=\"font-size:18px;\">25.1</t></center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_n</b><br>simple head</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.338</b><br><b>0.338</b><br>0.213<br>0.002</center></td>\n",
        "    <td><center><b>0.151</b><br>0.150<br>0.096<br>0.001</center></td>\n",
        "    <td><center><b>16.269 <i>~ 61 FPS</b><br>18.148 <i>~ 55 FPS<br>18.300 <i>~ 55 FPS<br>17.848 <i>~ 56 FPS</center></td>\n",
        "    <td><center>99.638<br>72.208<br>74.088<br><b>70.801</b></center></td>\n",
        "    <td><center><t style=\"font-size:18px;\">2.33</t></center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_n</b><br>decoupled head</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.353</b><br>0.349<br>0.250<br>0.006</center></td>\n",
        "    <td><center><b>0.165</b><br><b>0.165</b><br>0.102<br>0.002</center></td>\n",
        "    <td><center>21.629 <i>~ 46 FPS<br>23.407 <i>~ 43 FPS<br><b>20.935 <i>~ 48 FPS</b><br>21.515 <i>~ 46 FPS</center></td>\n",
        "    <td><center>119.691<br>89.465<br><b>82.417</b><br>90.662</center></td>\n",
        "    <td><center><t style=\"font-size:18px;\">2.8</t></center></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Finetuned official models**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th><center> Model </center></th>\n",
        "    <th><center>  </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50</t> </center></th>\n",
        "    <th><center> mAP<br><t style=\"font-size:12px;\">50-95</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b1</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> Speed<br><t style=\"font-size:12px;\">RTX3060 b16</t> <br><t style=\"font-size:12px;\">(ms)</t> </center></th>\n",
        "    <th><center> params <br><t style=\"font-size:12px;\">(M)</t> </center></th>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_m</b><br>(overfit)</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.707</b><br>0.706<br>0.642<br>0.346</center></td>\n",
        "    <td><center><b>0.376</b><br><b>0.376</b><br>0.334<br>0.164</center></td>\n",
        "    <td><center>17.057 <i>~ 58 FPS<br><b>14.213 <i>~ 70 FPS</b><br>14.229 <i>~ 70 FPS<br>14.714 <i>~ 68 FPS</center></td>\n",
        "    <td><center>242.669<br>145.945<br><b>143.637</b><br>150.257</td>\n",
        "    <td><center><t style=\"font-size:18px;\">21.17</t></center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_m</b><br>(best)</center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.556</b><br><b>0.556</b><br>0.539<br>0.320</center></td>\n",
        "    <td><center>0.275<br><b>0.276</b><br>0.261<br>0.137</center></td>\n",
        "    <td><center>*</center></td>\n",
        "    <td><center>*</center></td>\n",
        "    <td><center>*</center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center><i><b>YOLOv5_n</b></center></td>\n",
        "    <td> <i>base<br>fp16<br>0.3 pruning<br>0.5 pruning</td>\n",
        "    <td><center><b>0.469</b><br><b>0.469</b><br>0.293<br>0.063</center></td>\n",
        "    <td><center><b>0.225</b><br><b>0.225</b><br>0.133<br>0.020</center></td>\n",
        "    <td><center><b>7.559 <i>~ 132 FPS</b><br>8.528 <i>~ 117 FPS<br>8.623 <i>~ 116 FPS<br>9.214 <i>~ 108 FPS</center></td>\n",
        "    <td><center>50.021<br><b>33.307</b><br>34.161<br>33.778</center></td>\n",
        "    <td><center><t style=\"font-size:18px;\">1.86</t></center></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "McjXDHbZNROw",
        "FurxaRlU86tq",
        "KgUD3Gw764ei",
        "hXkZIRDeNw_q",
        "eJhhybij_bds"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sappia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "5f06e66338bb5301debc8a4cff3b178f3ee2a0c1aca00670585ce1ed8b6c95da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
