{"cells":[{"cell_type":"markdown","source":["**Cloning Repo**"],"metadata":{"id":"9yKx-PCLo1st"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAt_v8w-mCQy"},"outputs":[],"source":["!rm -rf visiope_project > /dev/null\n","!git clone https://github.com/lavallone/VISIOPE_project.git visiope_project"]},{"cell_type":"markdown","metadata":{"id":"ByWmvEvlsG6m"},"source":["# Data processing 🔨\n","\n","<a href=\"https://imgur.com/2kk27xI\"><img src=\"https://i.imgur.com/2kk27xI.png\" title=\"source: imgur.com\" width=150 height=130/></a> <a href=\"https://imgur.com/ZJTgfgb\"><img src=\"https://i.imgur.com/ZJTgfgb.png\" title=\"source: imgur.com\" width=170 height=90/></a> <a href=\"https://imgur.com/fO4AaCZ\"><img src=\"https://i.imgur.com/fO4AaCZ.png\" title=\"source: imgur.com\" width=150 height=130/></a>"]},{"cell_type":"markdown","source":["> ***Why data processing?***\n","\n","The goal of my project is to implement an object detection network that performs well both in term of accuracy and of inference time in a particular scenario: *the streets of Rome*. Due to the lack of annotated datasets of Italian cities for this task, the only solution was to integrate various datasets from the autonomous driving field in order to expose the model to as many situations as possible."],"metadata":{"id":"X_MJJakDrB6i"}},{"cell_type":"markdown","source":["> ***Which dataset should we use?*** \n","\n","In recent years many datasets dealing with *autonomous driving* grew drastically. But since most of the images share the similar scene content (because they are from the same video clip), which makes the model easy to over-fitting, it's necessary, in order to build a well generalized model, to use multiple datasets. So to tackle this issue and improve the detector, I decided to use:\n","- **Waymo** open dataset: very huge and collected well. The company has been working on autonomous vehicle technology for over a decade and is considered a leader in the field. Since it operates in San Francisco the environments was all similar and not appropriate to generalize in other settings.\n","- **BDD100K** (Berkeley DeepDrive 100K) is a large-scale diverse dataset for self-driving cars and computer vision research. It contains over 100,000 high-resolution video clips, capturing various driving scenarios in different weather conditions, times of day, and lighting levels. The dataset includes annotations for various objects, such as vehicles and pedestrians. The quality of the videos is not really amazing.\n","- **Argoverse-HD** dataset was created by Argo AI, a self-driving vehicle company, and is designed to be used for training and evaluating autonomous vehicle algorithms. The sample images are fewer with respect to the other two datasets. <br>*(it's been \"created\" for the Streaming Perception Challenge 2021, in which the YOLOX architecture won the first prize)*\n"],"metadata":{"id":"w3ChRY0vntfs"}},{"cell_type":"markdown","source":["> ***Which classes?***\n","\n","This is a very important question to answer. The three datasets have distinct annotated categories, so it's necessary to determine which ones will be the final ones for this project. I finally decided to have only three classes: **VEHICLE**, **PERSON** and **MOTORBIKE**. I didn't consider bicycles because in Rome they are very rare and it made more sense for the purpose of a possible *warning car system* to detect people that ride bicycles as PERSON. Always considering the \"real\" scenario in which I wat to test my model, I decided to consider the motorbike as a standalone vehicle, since the city is full of them and I'd like to detect them separately from cars!\n","\n"],"metadata":{"id":"m0zdOCtOy9UF"}},{"cell_type":"markdown","source":["> <a href=\"https://imgur.com/hs272pm\"><img src=\"https://i.imgur.com/hs272pm.png\" title=\"source: imgur.com\" width=15 height=15/></a> The images and annotations of the dataset are stored in *Google Drive* because to heavy to be on my local machine. "],"metadata":{"id":"AxxM7RnHySln"}},{"cell_type":"markdown","source":["> 🤯 The overall process took me a very long time to be completed. This because of the slowness of Google Drive uploading/downloading speed and of the  huge amount of data I dealt with!"],"metadata":{"id":"5Rbgm5IzHGvO"}},{"cell_type":"markdown","metadata":{"id":"Lyb_XY_AcNhI"},"source":["## **1** - Downloading raw data\n","`downloading phase`\n"]},{"cell_type":"markdown","metadata":{"id":"VVLcz8XtsLgl"},"source":["### Waymo <a href=\"https://imgur.com/2kk27xI\"><img src=\"https://i.imgur.com/2kk27xI.png\" title=\"source: imgur.com\" width=20 height=20/></a>"]},{"cell_type":"markdown","metadata":{"id":"M-hSV0YvzNci"},"source":["Instead of downloading the dataset on my local machine, then upload it to Google Drive and finally mount it to Colab and being able to access the data, I'll directly download it from Google Cloud buckets to my Colab machine. \n","\n","In order to do that, I first have to authenticate with GCP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYywBV0J0Feh"},"outputs":[],"source":["!gcloud auth login"]},{"cell_type":"markdown","metadata":{"id":"KNz_qFOw0IxH"},"source":["This command will ask you to run another command on a machine where a web browser can be launched (our local one).\n","\n","Do it and follow the instrunctions --> you can now run the 'copying command'.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFgKe7p_1E9G"},"outputs":[],"source":["# training files\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/training/training_0000.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/training/training_0001.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/training/training_0002.tar /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXlD-PmH2w5H"},"outputs":[],"source":["# testing files\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/testing/testing_0000.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/testing/testing_0001.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/testing/testing_0002.tar /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-yxYz5C2xIf"},"outputs":[],"source":["# validation files\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/validation/validation_0000.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/validation/validation_0001.tar /content\n","!gsutil cp gs://waymo_open_dataset_v_1_3_2/archived_files/validation/validation_0002.tar /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uwq0P_VW45b4"},"outputs":[],"source":["# saving the files to Google Drive takes longer, but this way I can save them 'permanently'!\n","!tar -xvf \"/content/training_000x.tar\" -C \"datasets/Waymo/images/tfrecord\"\n","!rm \"/content/training_000x.tar\"\n","!tar -xvf \"/content/testing_000x.tar\" -C \"dataset/Waymo/images/tfrecord\"\n","!rm \"/content/testing_000x.tar\"\n","!tar -xvf \"/content/validation_000x.tar\" -C \"dataset/Waymo/images/tfrecord\"\n","!rm \"/content/validation_000x.tar\""]},{"cell_type":"markdown","metadata":{"id":"xy2hj5eNV67y"},"source":["### BDD100K <a href=\"https://imgur.com/ZJTgfgb\"><img src=\"https://i.imgur.com/ZJTgfgb.png\" title=\"source: imgur.com\" width=40 height=20/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqaYcS89C6nR"},"outputs":[],"source":["%cd datasets/BDD100K/\n","\n","## TRAINING\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-1.zip\n","!unzip images20-track-train-1.zip\n","!rm images20-track-train-1.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-2.zip\n","!unzip images20-track-train-2.zip\n","!rm images20-track-train-2.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-3.zip\n","!unzip images20-track-train-3.zip\n","!rm images20-track-train-3.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-4.zip\n","!unzip images20-track-train-4.zip\n","!rm images20-track-train-4.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-5.zip\n","!unzip images20-track-train-5.zip\n","!rm images20-track-train-5.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-train-6.zip\n","!unzip images20-track-train-6.zip\n","!rm images20-track-train-6.zip\n","\n","# VALIDATION\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-val-1.zip\n","!unzip images20-track-val-1.zip\n","!rm images20-track-val-1.zip\n","\n","## TESTING (we don't download these data because we don't have their annotations!)\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-test-1.zip\n","!unzip images20-track-test-1.zip\n","!rm images20-track-test-1.zip\n","\n","!wget http://dl.yf.io/bdd100k/mot20/images20-track-test-2.zip\n","!unzip images20-track-test-2.zip\n","!rm images20-track-test-2.zip"]},{"cell_type":"markdown","metadata":{"id":"3j_PJ6Co2rFi"},"source":["🔄"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKgzsebwHAeB"},"outputs":[],"source":["# to synchronize Colab with Google Drive (since our drive is growing fastly).\n","# This has to be done when we're modifying our Drive and we want to be sure to have it updated.\n","from google.colab import drive\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"ky4uVubcsghd"},"source":["### Argoverse-HD <a href=\"https://imgur.com/fO4AaCZ\"><img src=\"https://i.imgur.com/fO4AaCZ.png\" title=\"source: imgur.com\" width=20 height=20/></a>"]},{"cell_type":"markdown","source":["It uses exactly the **COCO annotations**! The ones to which I'll convert the annotations of the other two datasets. \n","\n","*(also in this dataset the annotations for the test set are not provided)*"],"metadata":{"id":"PWos9lvT9E0q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7QNqyd_uoXC"},"outputs":[],"source":["# KAGGLE default stuffs to do in order to use the API\n","!pip install -q kaggle\n","!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aN4l_mPdvjAb"},"outputs":[],"source":["%cd datasets/Argoverse\n","!kaggle datasets download -d mtlics/argoversehd\n","!unzip argoversehd.zip\n","%cd /content"]},{"cell_type":"markdown","metadata":{"id":"DqWRRwIGJhck"},"source":["## **2** - Standardizing data \n","`building phase`"]},{"cell_type":"markdown","metadata":{"id":"SiIlbuYMAcJv"},"source":["Now, in order to be able to give as inputs all the data from these different datasets to the model, we need to *standardize* them (i.e. make their annotations to have the same format and their images to be of the same size). \n","We'll process only the data with annotations of each dataset. Each dataset subfolder is organised in this way:\n","       \n","\n","```\n","labels/\n","      / COCO\n","            / annotations.json # a unique json file for all the training images of the dataset\n","images/\n","      / videos\n","            /video_01\n","            /video_02\n","            ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"YygwNPKokUiT"},"source":["> All the images annotations will be converted in the ***COCO*** dataset format, since it's the most popular and gold standard dataset for detection tasks in computer vision. "]},{"cell_type":"markdown","metadata":{"id":"IFzRNR28c8iB"},"source":["### Waymo <a href=\"https://imgur.com/2kk27xI\"><img src=\"https://i.imgur.com/2kk27xI.png\" title=\"source: imgur.com\" width=20 height=20/></a>"]},{"cell_type":"markdown","metadata":{"id":"iHey6APrdNB0"},"source":["Since the dataset is saved as a set of *.tar* archives which contain *tfRecord* files, we need to make a preprocessing phase in order to extrapolate the images and their corresponding labels.\n","\n","*(we download the first 3 .tar files from the dataset)*\n","\n","We're able to do that thanks to the modified version of the toolkit developed by Kushal B Kusram. <br>https://github.com/KushalBKusram/WaymoOpenDatasetToolKit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxrqzcVEm65v"},"outputs":[],"source":["!pip3 install waymo-open-dataset-tf-2-1-0==1.2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShhN3LwuzRaJ"},"outputs":[],"source":["# if it's needed to initialize the json label dictionary\n","import json\n","d = {\"info\" : {\"num_videos\": 116, \"num_images\": 68760}, \"images\" : [], \"categories\": [ {\"name\" : \"vehicle\", \"id\" : 0}, {\"name\" : \"person\", \"id\" : 1}, {\"name\" : \"motorbike\", \"id\" : 2}], \"annotations\": []}\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/Waymo/labels/COCO/annotations.json\", \"w\")\n","json.dump(d, f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm_UJGdAv14t"},"outputs":[],"source":["%cd /content/visiope_project\n","!git pull\n","\n","!python data_toolkit/building/build.py \"waymo\""]},{"cell_type":"markdown","metadata":{"id":"ErWDXS5udI6B"},"source":["### BDD100K <a href=\"https://imgur.com/ZJTgfgb\"><img src=\"https://i.imgur.com/ZJTgfgb.png\" title=\"source: imgur.com\" width=40 height=20/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2VRs8KoZ-co"},"outputs":[],"source":["## [IF we want to use the BDD100K Toolkit] ##\n","!rm -rf bdd100k_toolkit > /dev/null\n","!git clone https://github.com/bdd100k/bdd100k.git bdd100k_toolkit\n","%cd bdd100k_toolkit\n","!pip3 install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4mPCVWOBnKz"},"outputs":[],"source":["# If it's needed to initialize the json label dictionary\n","import json\n","d = {\"info\" : {\"num_videos\": 1400, \"num_images\": 277594}, \"images\" : [], \"categories\": [ {\"name\" : \"vehicle\", \"id\" : 0}, {\"name\" : \"person\", \"id\" : 1}, {\"name\" : \"motorbike\", \"id\" : 2}], \"annotations\": []}\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/BDD100K/labels/COCO/annotations.json\", \"w\")\n","json.dump(d, f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rxgYztmvc5B"},"outputs":[],"source":["%cd /content/visiope_project\n","!git pull\n","\n","# now that we filter less annotations, the label json file became huge! --> we found a way to improve the efficency of the code! ;)\n","!python data_toolkit/building/build.py \"bdd100k\""]},{"cell_type":"markdown","metadata":{"id":"n5UABSDUa1tZ"},"source":["### Argoverse-HD <a href=\"https://imgur.com/fO4AaCZ\"><img src=\"https://i.imgur.com/fO4AaCZ.png\" title=\"source: imgur.com\" width=20 height=20/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJZ-jgOF52mM"},"outputs":[],"source":["!rm -rf cocoapi > /dev/null\n","!git clone https://github.com/cocodataset/cocoapi.git cocoapi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QYmtaP23sgc"},"outputs":[],"source":["import json\n","# we need to add the 'info' key to the dictionary\n","ris_dict = { \"info\" : {\"num_videos\": 89, \"num_images\": 54446} , \"categories\" : None, \"images\": None, \"annotations\" : None, \"sequences\" : None, \"seq_dirs\" : None, \"coco_subset\" : None, \"coco_mapping\" : None, \"n_tracks\" : None}\n","d = json.load(open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse/labels/old_train.json\"))\n","for k in d.keys():\n","  ris_dict[k] = d[k]\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse/labels/old_train.json\", \"w\")\n","json.dump(ris_dict, f)\n","f.close()\n","ris_dict = { \"info\" : {\"num_videos\": 89, \"num_images\": 39408} , \"categories\" : None, \"images\": None, \"annotations\" : None, \"sequences\" : None, \"seq_dirs\" : None, \"coco_subset\" : None, \"coco_mapping\" : None, \"n_tracks\" : None}\n","d = json.load(open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse/labels/old_val.json\"))\n","for k in d.keys():\n","  ris_dict[k] = d[k]\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse/labels/old_val.json\", \"w\")\n","json.dump(ris_dict, f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbtYtuf7aHiN"},"outputs":[],"source":["%cd /content/visiope_project\n","!git pull\n","\n","!python data_toolkit/building/build.py \"argoverse\""]},{"cell_type":"markdown","metadata":{"id":"a917aoRU6EWm"},"source":["## Data summary 📈"]},{"cell_type":"markdown","source":["> After this process, we can visualize how many images and videos we have standardized from the three datasets. And if the annotations are \"*COCO compatible*\"."],"metadata":{"id":"D0XvFCjBFnUt"}},{"cell_type":"markdown","metadata":{"id":"j-saCYYtkmIx"},"source":["#### Number of videos and images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0-S29bIkt6o"},"outputs":[],"source":["import os\n","waymo = \"/content/drive/MyDrive/VISIOPE/Project/datasets/Waymo\"\n","bdd100k = \"/content/drive/MyDrive/VISIOPE/Project/datasets/BDD100K\"\n","argoverse = \"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse\"\n","\n","l = [waymo, bdd100k, argoverse]\n","ris = { \"Waymo\" : [0,0], \"BDD100K\" : [0,0], \"Argoverse\" : [0,0] }\n","\n","for dir in l:\n","  num_train_videos = 0\n","  num_train_images = 0\n","  for v in os.listdir(dir+\"/images/videos\"):\n","    num_train_videos = num_train_videos + 1\n","    num_train_images = num_train_images + len(os.listdir(dir+\"/images/videos/\"+v))\n","  ris[dir[48:]][0] = num_train_videos\n","  ris[dir[48:]][1] = num_train_images\n","\n","print(ris) # --> {'Waymo': [116, 68760], 'BDD100K': [1400, 277594], 'Argoverse': [89, 54446]}"]},{"cell_type":"markdown","metadata":{"id":"yDaGLi2YkyIf"},"source":["                                            Waymo          BDD100K           Argoverse\n","                    Number of videos        116            1400              89\n","                    Number of images        68760          277594            54446"]},{"cell_type":"markdown","metadata":{"id":"oqU70iQK3yXo"},"source":["#### COCO compatibility"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22025,"status":"ok","timestamp":1667234920982,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"},"user_tz":-60},"id":"Ah6m3Bb8KnrN","outputId":"c52db283-bd95-4061-8fc0-b1037df3ee7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=3.22s)\n","creating index...\n","index created!\n","Waymo\n","68760\n","68760\n","------------\n","loading annotations into memory...\n","Done (t=13.44s)\n","creating index...\n","index created!\n","BDD100K\n","277594\n","277594\n","------------\n","loading annotations into memory...\n","Done (t=3.91s)\n","creating index...\n","index created!\n","Argoverse\n","54446\n","54446\n"]}],"source":["# To check if the json files created are compatible with COCO format...\n","\n","from pycocotools.coco import COCO\n","waymo_json =     \"/content/drive/MyDrive/VISIOPE/Project/datasets/Waymo/labels/COCO/annotations.json\"\n","bdd100k_json =   \"/content/drive/MyDrive/VISIOPE/Project/datasets/BDD100K/labels/COCO/annotations.json\"\n","argoverse_json = \"/content/drive/MyDrive/VISIOPE/Project/datasets/Argoverse/labels/COCO/annotations.json\"\n","coco_1 = COCO(waymo_json)\n","print(\"Waymo\")\n","print(len(coco_1.dataset[\"images\"]))\n","print(len(coco_1.getImgIds())) # it returns the number of unique image ids!\n","print(\"------------\")\n","coco_2 = COCO(bdd100k_json)\n","print(\"BDD100K\")\n","print(len(coco_2.dataset[\"images\"]))\n","print(len(coco_2.getImgIds()))\n","print(\"------------\")\n","coco_3 = COCO(argoverse_json)\n","print(\"Argoverse\")\n","print(len(coco_3.dataset[\"images\"]))\n","print(len(coco_3.getImgIds()))"]},{"cell_type":"markdown","metadata":{"id":"a-q0VN_hp4Gp"},"source":["## **3** - Custom dataset creation\n","`extraction phase`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2GGM52DTAW0"},"outputs":[],"source":["# initializing the processed_images_so_far.json file\n","import json\n","d = {\"images_so_far\" : [0]}\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/data/processed_images_so_far.json\", \"w\")\n","json.dump(d, f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EOvIbc10MSi"},"outputs":[],"source":["%cd /content/visiope_project\n","!git pull\n","\n","!python data_toolkit/extraction/main.py"]},{"cell_type":"markdown","metadata":{"id":"d2m7LfdytTaw"},"source":["<table>\n","  <tr>\n","    <th><center> </center></th>\n","    <th><center>Before</center></th>\n","    <th><center>After</center></th>\n","    <td><center><i>(the extraction phase)</center></td>\n","  </tr>\n","  <tr>\n","    <td><center><i>Number of images</center></td>\n","    <td><center>400.800</center></td>\n","    <td><center>125.431</center></td>\n","  </tr>\n","  <tr>\n","    <td><center><i>Number of annotations</center></td>\n","    <td><center>3.795.457</center></td>\n","    <td><center>1.133.294</center></td>\n","  </tr>\n","</table>\n","\n","> *Images reduction* of the ***68,71%*** *!* <br> *Annotations reduction* of the ***70.15%*** *!*"]},{"cell_type":"markdown","metadata":{"id":"fzv1QNRgh92_"},"source":["❌ *Not all the images have annotated bounding boxes! <br> For semplicity we deleted all the ones that have NO annotations.* <br> *(from **125431** to **114126** images)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_C06XdmorpXw"},"outputs":[],"source":["# code run to achieve it\n","import json\n","from tqdm import tqdm\n","\n","images_list= (json.load(open(\"/content/drive/MyDrive/VISIOPE/Project/data/images_list.json\")))[\"images_list\"]\n","annotations = (json.load(open(\"/content/drive/MyDrive/VISIOPE/Project/data/labels/COCO/annotations.json\")))\n","\n","images_annotated = [str(int(ann[\"image_id\"])) for ann in annotations[\"annotations\"]] # 1133294 of annotations\n","images_annotated = list(set(images_annotated)) # 114126\n","print(\"Total number of images:\")\n","print(len(images_list))\n","print(\"Total number of annotated images:\")\n","print(len(images_annotated))\n","\n","# modify annotations.json\n","annotations[\"info\"][\"num_images\"] = len(images_annotated)\n","images = list(filter(lambda x: str(int(x[\"id\"])) in images_annotated, tqdm(annotations[\"images\"]) ))\n","annotations[\"images\"] = images\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/data/labels/COCO/annotations.json\", \"w\")\n","json.dump(annotations, f)\n","f.close()\n","\n","# modify images_list.json\n","img2id = json.load(open(\"/content/drive/MyDrive/VISIOPE/Project/data/lookup_tables/img2id.json\"))\n","filtered_images = list(filter(lambda x: str(int(img2id[x])) in images_annotate#d, tqdm(images_list)))\n","print(\"Number of annotated images after the filtering:\")\n","print(len(filtered_images))\n","\n","d = {\"images_list\" : filtered_images}\n","f = open(\"/content/drive/MyDrive/VISIOPE/Project/data/images_list.json\", \"w\")\n","json.dump(d, f)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"Xbh76EOL5_pr"},"source":["> We just have to zip all images andannotation and produce a ***data.zip*** always available for faster downloads!"]},{"cell_type":"markdown","source":["## 💡 *Observations during the process*"],"metadata":{"id":"XPQAMNUvPhZk"}},{"cell_type":"markdown","metadata":{"id":"PdvqSxRoenN5"},"source":["Since our final purpose is to test the detection system in the real world, in our case the streets of Rome, we need to think about which are the useful labels and which not. \n","In general, three are the main aspects that have to be discussed to improve the quality of our data:\n","\n","*   \"Fine-tuning\" the selection mechanism of annotations for each datasets. *Which annotations does worth it to stay?*\n","It is important because, for example, the first version of the BDD100K annotations was too strict. The filtering was too severe and most of the bounding boxes were discarded.\n","```\n","Infos about the quality of annotations:\n","Waymo:        difficulty level --> {LEVEL_1, LEVEL2}\n","BDD100K:      crowd, occluded, truncated\n","Argoverse:    is_crowd \n","```\n","\n","\n","*   Merging the class of buses, trucks and cars into the class **VEHICLE**.\n"," In order to have a easier detection task and because the Waymo dataset doesn't make any difference between trucks, buses and cars, we simply merge these 3 classes into the vehicle one.\n","*   Dealing with people that rides a bicycle or a motorcycle. Some datasets consider the two-wheels vehicle and the rider as two different entities, others not. Do we merge bikes and motorbikes into a single class? \n","This was the most intruing point. Since Rome is full of motorbikes and less full of bicycles, and because our model will be trained on data where motorbikes are rare, we suppose that our system won't work properly with \"*two wheels vehicles*\". With the goal of overcoming this issue we took this decision:\n","  *   All the detected bicycles (so their riders) will be considered as **PERSON**. The  Waymo \"*TYPE_CYCLIST*\" or the \"*rider*\" BDD100K class for example.\n","  *   Since Rome is full of them, motorbikes are considered as a single category (**MOTORBIKE**).\n","\n","```\n","Different classes of each dataset:\n","Waymo:         TYPE_VEHICLE, TYPE_PEDESTRIAN, TYPE_CYCLIST\n","BDD100K:       pedestrian, rider, car, truck, bus, motorcycle, bicycle\n","Argoverse:     person, bicycle, car, motorcycle, bus, truck\n","```"]}],"metadata":{"colab":{"collapsed_sections":["Lyb_XY_AcNhI","VVLcz8XtsLgl","xy2hj5eNV67y","ky4uVubcsghd","IFzRNR28c8iB","ErWDXS5udI6B","n5UABSDUa1tZ","a917aoRU6EWm","oqU70iQK3yXo","a-q0VN_hp4Gp","XPQAMNUvPhZk"],"machine_shape":"hm","provenance":[],"mount_file_id":"1sCqnwYm9Dodk1YodD1asVpRMBBdT-8r1","authorship_tag":"ABX9TyN9uHr8h0REWSaMZCbIRstE"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}